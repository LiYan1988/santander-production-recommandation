{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Preprocessing Results with [Kernel](https://www.kaggle.com/sudalairajkumar/when-less-is-more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.name=='nt':\n",
    "    try:\n",
    "        mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-8.1.0-posix-seh-rt_v6-rev0\\\\mingw64\\\\bin'\n",
    "        os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "tqdm.tqdm.pandas()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_hdf('../input/train_ver3.hdf', 'train_ver3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_hdf('../input/test_ver3.hdf', 'test_ver3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ncodpers',\n",
    " 'canal_entrada',\n",
    " 'conyuemp',\n",
    " 'ind_actividad_cliente',\n",
    " 'ind_empleado',\n",
    " 'ind_nuevo',\n",
    " 'indext',\n",
    " 'indfall',\n",
    " 'indrel',\n",
    " 'indrel_1mes',\n",
    " 'indresi',\n",
    " 'pais_residencia',\n",
    " 'segmento',\n",
    " 'sexo',\n",
    " 'tipodom',\n",
    " 'tiprel_1mes',\n",
    " 'age',\n",
    " 'antiguedad',\n",
    " 'renta']\n",
    "\n",
    "target_cols = ['ind_cco_fin_ult1',\n",
    " 'ind_cder_fin_ult1',\n",
    " 'ind_cno_fin_ult1',\n",
    " 'ind_ctju_fin_ult1',\n",
    " 'ind_ctma_fin_ult1',\n",
    " 'ind_ctop_fin_ult1',\n",
    " 'ind_ctpp_fin_ult1',\n",
    " 'ind_deco_fin_ult1',\n",
    " 'ind_dela_fin_ult1',\n",
    " 'ind_deme_fin_ult1',\n",
    " 'ind_ecue_fin_ult1',\n",
    " 'ind_fond_fin_ult1',\n",
    " 'ind_hip_fin_ult1',\n",
    " 'ind_nom_pens_ult1',\n",
    " 'ind_nomina_ult1',\n",
    " 'ind_plan_fin_ult1',\n",
    " 'ind_pres_fin_ult1',\n",
    " 'ind_reca_fin_ult1',\n",
    " 'ind_recibo_ult1',\n",
    " 'ind_tjcr_fin_ult1',\n",
    " 'ind_valo_fin_ult1',\n",
    " 'ind_viv_fin_ult1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2015-05-28 and 2015-06-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.loc[train.fecha_dato=='2015-05-28']\n",
    "train2 = train.loc[train.fecha_dato=='2015-06-28']\n",
    "\n",
    "# products in 2015-06-28\n",
    "target = train2.loc[:, ['ncodpers']+target_cols].copy()\n",
    "target.set_index('ncodpers', inplace=True, drop=False)\n",
    "# a dataframe containing the ncodpers only\n",
    "target_ncodpers = pd.DataFrame(target.ncodpers)\n",
    "# drop ncodpers from target\n",
    "target.drop('ncodpers', axis=1, inplace=True)\n",
    "\n",
    "# products in 2015-05-28\n",
    "prev_target = train1.loc[:, ['ncodpers']+target_cols].copy()\n",
    "prev_target.set_index('ncodpers', inplace=True, drop=True)\n",
    "# join to target_ncodpers, so that prev_target includes all customers in 2015-05-28\n",
    "prev_target = target_ncodpers.join(prev_target, how='left')\n",
    "prev_target.fillna(0.0, inplace=True)\n",
    "prev_target.drop('ncodpers', axis=1, inplace=True)\n",
    "\n",
    "# new products in 2015-06-28\n",
    "target = target.subtract(prev_target)\n",
    "target[target<0] = 0\n",
    "\n",
    "# train set of 2015-06-28 includes customer features and products in 2015-05-28\n",
    "x_vars = train2[cat_cols].copy()\n",
    "x_vars.reset_index(inplace=True, drop=True)\n",
    "x_vars.reset_index(inplace=True, drop=False)\n",
    "x_vars_cols = x_vars.columns.tolist()\n",
    "x_vars_cols[0] = 'sample_order'\n",
    "x_vars.columns = x_vars_cols\n",
    "x_vars.set_index('ncodpers', drop=True, inplace=True)\n",
    "x_vars = x_vars.join(prev_target)\n",
    "\n",
    "# get samples for each new product\n",
    "\n",
    "# join target to x_vars\n",
    "x_vars_new = x_vars.join(target, rsuffix='_t')\n",
    "\n",
    "# set ncodpers as one column\n",
    "x_vars_new.reset_index(inplace=True)\n",
    "x_vars.reset_index(inplace=True)\n",
    "\n",
    "# melt\n",
    "x_vars_new = x_vars_new.melt(id_vars=x_vars.columns)\n",
    "# mapping from target_cols to index\n",
    "target_cols_mapping = {c+'_t': n for (n, c) in enumerate(target_cols)}\n",
    "# replace column name by index\n",
    "x_vars_new.variable.replace(target_cols_mapping, inplace=True)\n",
    "# reorder rows\n",
    "x_vars_new.sort_values(['sample_order', 'variable'], inplace=True)\n",
    "# keep new products\n",
    "x_vars_new = x_vars_new[x_vars_new.value>0]\n",
    "# drop value\n",
    "x_vars_new.drop(['sample_order', 'value'], inplace=True, axis=1)\n",
    "# keep the order of samples as in the original data set\n",
    "x_vars_new.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# variables\n",
    "x_vars = x_vars_new.iloc[:, :-1].copy()\n",
    "# target\n",
    "target = x_vars_new.iloc[:, [0, -1]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016-05-28 and 2016-06-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.loc[train.fecha_dato=='2016-05-28']\n",
    "train2 = test.loc[test.fecha_dato=='2016-06-28']\n",
    "\n",
    "# products in 2016-05-28\n",
    "prev_target = train1.loc[:, ['ncodpers']+target_cols].copy()\n",
    "prev_target.set_index('ncodpers', inplace=True, drop=True)\n",
    "\n",
    "prev_target = target_ncodpers.join(prev_target, how='left')\n",
    "prev_target.fillna(0.0, inplace=True)\n",
    "prev_target.drop('ncodpers', axis=1, inplace=True)\n",
    "\n",
    "# train set of 2016-06-28 includes customer features and products in 2016-05-28\n",
    "x_vars_test = train2[cat_cols].copy()\n",
    "x_vars_test.set_index('ncodpers', drop=False, inplace=True)\n",
    "x_vars_test = x_vars_test.join(prev_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.91645\n",
      "[1]\ttrain-mlogloss:2.78563\n",
      "[2]\ttrain-mlogloss:2.6751\n",
      "[3]\ttrain-mlogloss:2.58366\n",
      "[4]\ttrain-mlogloss:2.49453\n",
      "[5]\ttrain-mlogloss:2.41242\n",
      "[6]\ttrain-mlogloss:2.34612\n",
      "[7]\ttrain-mlogloss:2.28669\n",
      "[8]\ttrain-mlogloss:2.22937\n",
      "[9]\ttrain-mlogloss:2.17384\n",
      "[10]\ttrain-mlogloss:2.12307\n",
      "[11]\ttrain-mlogloss:2.08031\n",
      "[12]\ttrain-mlogloss:2.04\n",
      "[13]\ttrain-mlogloss:1.99893\n",
      "[14]\ttrain-mlogloss:1.95945\n",
      "[15]\ttrain-mlogloss:1.92307\n",
      "[16]\ttrain-mlogloss:1.88998\n",
      "[17]\ttrain-mlogloss:1.85772\n",
      "[18]\ttrain-mlogloss:1.82859\n",
      "[19]\ttrain-mlogloss:1.80279\n",
      "[20]\ttrain-mlogloss:1.77791\n",
      "[21]\ttrain-mlogloss:1.75428\n",
      "[22]\ttrain-mlogloss:1.72977\n",
      "[23]\ttrain-mlogloss:1.70668\n",
      "[24]\ttrain-mlogloss:1.68579\n",
      "[25]\ttrain-mlogloss:1.66468\n",
      "[26]\ttrain-mlogloss:1.6446\n",
      "[27]\ttrain-mlogloss:1.62652\n",
      "[28]\ttrain-mlogloss:1.61053\n",
      "[29]\ttrain-mlogloss:1.59396\n",
      "[30]\ttrain-mlogloss:1.57689\n",
      "[31]\ttrain-mlogloss:1.56132\n",
      "[32]\ttrain-mlogloss:1.54642\n",
      "[33]\ttrain-mlogloss:1.5324\n",
      "[34]\ttrain-mlogloss:1.5179\n",
      "[35]\ttrain-mlogloss:1.5043\n",
      "[36]\ttrain-mlogloss:1.49227\n",
      "[37]\ttrain-mlogloss:1.48045\n",
      "[38]\ttrain-mlogloss:1.46843\n",
      "[39]\ttrain-mlogloss:1.45647\n",
      "[40]\ttrain-mlogloss:1.44583\n",
      "[41]\ttrain-mlogloss:1.43534\n",
      "[42]\ttrain-mlogloss:1.42487\n",
      "[43]\ttrain-mlogloss:1.4151\n",
      "[44]\ttrain-mlogloss:1.40488\n",
      "[45]\ttrain-mlogloss:1.39618\n",
      "[46]\ttrain-mlogloss:1.38665\n",
      "[47]\ttrain-mlogloss:1.37767\n",
      "[48]\ttrain-mlogloss:1.36934\n",
      "[49]\ttrain-mlogloss:1.36178\n"
     ]
    }
   ],
   "source": [
    "param = {'objective': 'multi:softprob', \n",
    "         'eta': 0.05, \n",
    "         'max_depth': 8, \n",
    "         'silent': 0, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'mlogloss',\n",
    "         'min_child_weight': 1,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 0}\n",
    "num_rounds = 50\n",
    "\n",
    "dtrain = xgb.DMatrix(x_vars.values, target.values[:, 1])\n",
    "model = xgb.train(param, dtrain, num_rounds, evals=[(dtrain, 'train')], verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction from my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(xgb.DMatrix(x_vars_test.values))\n",
    "preds = np.argsort(preds, axis=1)\n",
    "preds = np.fliplr(preds)[:, :7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction from kernel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_r = np.load('preds_raw.npy')\n",
    "preds_r = np.argsort(preds_r, axis=1)\n",
    "preds_r = np.fliplr(preds_r)[:, :7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between the two models\n",
    "\n",
    "Note that the difference is not zero even though random seeds are the same in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = preds-preds_r\n",
    "a = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out prediction results from my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = x_vars_test.loc[:, 'ncodpers'].values\n",
    "final_preds = [' '.join([target_cols[k] for k in pred]) for pred in preds]\n",
    "\n",
    "out_df = pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds})\n",
    "out_df.to_csv('eda_4_6.csv.gz', compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
