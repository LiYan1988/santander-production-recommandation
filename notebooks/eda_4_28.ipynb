{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and CV based Winners' Solutions\n",
    "\n",
    "continued from eda_4_26\n",
    "\n",
    "New in this notebook:\n",
    "- average of products for each (customer, product) pair\n",
    "- exponent weighted average of products each (customer, product) pair\n",
    "- time since presence of products, distance to the first 1\n",
    "- time to the last positive flank (01)\n",
    "- time to the last negative flank (10)\n",
    "- time to the last 1, to the nearest product purchase\n",
    "- time to the first 1, to the first product purchase\n",
    "\n",
    "Trained@2015-06-28, validated@2015-12-28, mlogloss=1.28481\n",
    "\n",
    "Private score: 0.0302054, public score: 0.0298683\n",
    "\n",
    "To-do: \n",
    "- mean encoding of products grouped by combinations of: canal_entrada, segmento, cod_prov\n",
    "- Time since change and lags for a few non-product features: \n",
    "    - segmento\n",
    "    - ind_actividad_cliente\n",
    "    - cod_prov\n",
    "    - canal_entrada\n",
    "    - indrel_1mes\n",
    "    - tiprel_1mes\n",
    "\n",
    "\n",
    "Features:\n",
    "- before eda_4_25\n",
    "    - customer info in the second month\n",
    "    - products in the first month\n",
    "    - combination of first and second month `ind_actividad_cliente`\n",
    "    - combination of first and second month `tiprel_1mes`\n",
    "    - combination of first month product by using binary number (`target_combine`)\n",
    "    - encoding `target_combine` with \n",
    "        - mean number of new products\n",
    "        - mean number of customers with new products\n",
    "        - mean number of customers with each new products\n",
    "    - Count patterns in the last `max_lag` months\n",
    "    - Number of month to the last time the customer purchase each product\n",
    "        - CV@2015-12-28: mlogloss=1.29349\n",
    "        - Private score: 0.0302475, public score: 0.0299266\n",
    "- eda_4_25\n",
    "    - Use all available history data\n",
    "        - E.g., for 2016-05-28 train data, use all previous months, for 2015-02-28, use 1 lag month. \n",
    "        - Need to create test set that use the same amount of previous months for each training data set. \n",
    "        - This is from [the second winner's solution](https://www.kaggle.com/c/santander-product-recommendation/discussion/26824), his bold part in paragraph 4.\n",
    "    - Combine models trained on 2016-05-28 and 2015-06-28:\n",
    "        - Private score: 0.0304583, public score: 0.0300839\n",
    "        - This is to catch both seasonality and trend, presented in 2015-06-28 and 2016-05-28, respectively. \n",
    "        - This idea is mentioned by many winners, like [11-th winner](https://www.kaggle.com/c/santander-product-recommendation/discussion/26823) and [14-th winner](https://www.kaggle.com/c/santander-product-recommendation/discussion/26808)\n",
    "\n",
    "- eda_4_27\n",
    "    - put 2015-06-28 and 2016-05-28 in the same data set, with the same lag=5\n",
    "        - Private score:0.0303096, public score: 0.0299867\n",
    "        - Different as [11-th winner's discussion](https://www.kaggle.com/c/santander-product-recommendation/discussion/26823)\n",
    "            > We tested this by adding 50% of May-16 data to our June model and sure enough, we went from 0.0301 to 0.0303. Then, we built separate models for Jun and May, but the ensemble didn’t work. We weren’t surprised because June data is better for seasonal products, and May data is better for trend products. And vice-versa, June data is bad for trend products and May data is bad for seasonal products. So, they sort of cancelled each other out.\n",
    "\n",
    "        - But my score is always worse than theirs, maybe this is the reason why we have different observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from santander_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_xgb_skfrm(params, x_train, y_train, num_boost_round=3, n_splits=3, \n",
    "                           n_repeats=2, random_state=0, verbose_eval=False):\n",
    "    '''\n",
    "    CV of xgb using Stratified KFold Repeated Models (SKFRM)\n",
    "    verbose_eval is the same as in xgb.train\n",
    "    '''\n",
    "    cv_results = {}\n",
    "    clfs = {}\n",
    "    running_time = {}\n",
    "    \n",
    "    eval_metric = params['eval_metric']\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=np.random.randint(10**6), shuffle=True)\n",
    "    \n",
    "    for m in range(n_repeats):\n",
    "        for n, (train_index, val_index) in enumerate(skf.split(x_train, y_train)):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Construct DMatrix\n",
    "            dtrain = xgb.DMatrix(x_train.iloc[train_index], label=y_train.iloc[train_index])\n",
    "            dval = xgb.DMatrix(x_train.iloc[val_index], label=y_train.iloc[val_index])\n",
    "            \n",
    "            # Placeholder for evals_result\n",
    "            cv_results[m, n] = {}\n",
    "            params['seed'] = np.random.randint(10**6)\n",
    "            clfs[m, n] = xgb.train(params, dtrain, num_boost_round=num_boost_round,\n",
    "                                   evals=[(dtrain, 'train'), (dval, 'val')], \n",
    "                                   maximize=True, early_stopping_rounds=None, \n",
    "                                   evals_result=cv_results[m, n], verbose_eval=verbose_eval)\n",
    "        \n",
    "            running_time[m, n] = time.time() - start_time\n",
    "            \n",
    "            print('Repeat {}, split {}, validate score = {:.3f}, running time = {:.3f} min'.format(m, n, \n",
    "                cv_results[m, n]['val'][eval_metric][-1], running_time[m, n]/60))\n",
    "        \n",
    "    # Post-process cv_results\n",
    "    cv_results_final = {}\n",
    "    for m in range(n_repeats):\n",
    "        for n in range(n_splits):\n",
    "            cv_results_final['train', m, n] = cv_results[m, n]['train'][eval_metric]\n",
    "            cv_results_final['val', m, n] = cv_results[m, n]['val'][eval_metric]\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(cv_results_final)\n",
    "    df.index.name = 'iteration'\n",
    "    df.columns.names = ['dataset', 'repeat', 'split']\n",
    "\n",
    "    print('Score mean = {:.3f}, std = {:.3f}'.format(df['val'].iloc[-1].mean(), df['val'].iloc[-1].std()))\n",
    "    \n",
    "    return df, clfs, running_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0, split 0, validate score = 0.995, running time = 2.352 min\n",
      "Repeat 0, split 1, validate score = 0.999, running time = 2.313 min\n",
      "Repeat 0, split 2, validate score = 0.997, running time = 2.306 min\n",
      "Repeat 0, split 3, validate score = 0.998, running time = 2.321 min\n",
      "Repeat 1, split 0, validate score = 0.996, running time = 2.384 min\n",
      "Repeat 1, split 1, validate score = 0.999, running time = 2.300 min\n",
      "Repeat 1, split 2, validate score = 0.999, running time = 2.336 min\n",
      "Repeat 1, split 3, validate score = 0.997, running time = 2.344 min\n"
     ]
    }
   ],
   "source": [
    "x_train_june15, y_train_june15 = create_train('2015-06-28', pattern_flag=True)\n",
    "\n",
    "params = {'objective': 'multi:softprob', \n",
    "         'eta': 0.05, \n",
    "         'max_depth': 8, \n",
    "         'silent': 1, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'mlogloss',\n",
    "         'min_child_weight': 1,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 0}\n",
    "\n",
    "df, clfs, running_time = cv_xgb_skfrm(params, x_train_june15, y_train_june15, \n",
    "                                      verbose_eval=False, num_boost_round=100, \n",
    "                                      n_splits=4, n_repeats=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.705467</td>\n",
       "      <td>2.711566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.531360</td>\n",
       "      <td>2.542907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.392536</td>\n",
       "      <td>2.409143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset       train       val\n",
       "iteration                    \n",
       "0          2.705467  2.711566\n",
       "1          2.531360  2.542907\n",
       "2          2.392536  2.409143"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean(axis=1, level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-06-28, max_lag=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.70361\tdval-mlogloss:2.73293\n",
      "[1]\ttrain-mlogloss:2.53061\tdval-mlogloss:2.57631\n",
      "[2]\ttrain-mlogloss:2.39202\tdval-mlogloss:2.45619\n",
      "[3]\ttrain-mlogloss:2.27645\tdval-mlogloss:2.35394\n",
      "[4]\ttrain-mlogloss:2.1773\tdval-mlogloss:2.26842\n",
      "[5]\ttrain-mlogloss:2.09068\tdval-mlogloss:2.18941\n",
      "[6]\ttrain-mlogloss:2.01344\tdval-mlogloss:2.12208\n",
      "[7]\ttrain-mlogloss:1.94366\tdval-mlogloss:2.06529\n",
      "[8]\ttrain-mlogloss:1.88058\tdval-mlogloss:2.00993\n",
      "[9]\ttrain-mlogloss:1.82338\tdval-mlogloss:1.95961\n",
      "[10]\ttrain-mlogloss:1.77048\tdval-mlogloss:1.91311\n",
      "[11]\ttrain-mlogloss:1.72147\tdval-mlogloss:1.86929\n",
      "[12]\ttrain-mlogloss:1.67601\tdval-mlogloss:1.83026\n",
      "[13]\ttrain-mlogloss:1.63373\tdval-mlogloss:1.79266\n",
      "[14]\ttrain-mlogloss:1.59426\tdval-mlogloss:1.75835\n",
      "[15]\ttrain-mlogloss:1.55716\tdval-mlogloss:1.72968\n",
      "[16]\ttrain-mlogloss:1.52252\tdval-mlogloss:1.6984\n",
      "[17]\ttrain-mlogloss:1.49021\tdval-mlogloss:1.67133\n",
      "[18]\ttrain-mlogloss:1.45926\tdval-mlogloss:1.64782\n",
      "[19]\ttrain-mlogloss:1.43075\tdval-mlogloss:1.62538\n",
      "[20]\ttrain-mlogloss:1.40356\tdval-mlogloss:1.60261\n",
      "[21]\ttrain-mlogloss:1.37803\tdval-mlogloss:1.58191\n",
      "[22]\ttrain-mlogloss:1.35349\tdval-mlogloss:1.56007\n",
      "[23]\ttrain-mlogloss:1.33041\tdval-mlogloss:1.5418\n",
      "[24]\ttrain-mlogloss:1.30843\tdval-mlogloss:1.5233\n",
      "[25]\ttrain-mlogloss:1.28792\tdval-mlogloss:1.5062\n",
      "[26]\ttrain-mlogloss:1.26825\tdval-mlogloss:1.48971\n",
      "[27]\ttrain-mlogloss:1.24952\tdval-mlogloss:1.4726\n",
      "[28]\ttrain-mlogloss:1.23162\tdval-mlogloss:1.45768\n",
      "[29]\ttrain-mlogloss:1.21441\tdval-mlogloss:1.44338\n",
      "[30]\ttrain-mlogloss:1.1978\tdval-mlogloss:1.42925\n",
      "[31]\ttrain-mlogloss:1.18225\tdval-mlogloss:1.41652\n",
      "[32]\ttrain-mlogloss:1.1671\tdval-mlogloss:1.40453\n",
      "[33]\ttrain-mlogloss:1.15286\tdval-mlogloss:1.39168\n",
      "[34]\ttrain-mlogloss:1.13929\tdval-mlogloss:1.37966\n",
      "[35]\ttrain-mlogloss:1.12611\tdval-mlogloss:1.36813\n",
      "[36]\ttrain-mlogloss:1.11373\tdval-mlogloss:1.35737\n",
      "[37]\ttrain-mlogloss:1.10193\tdval-mlogloss:1.34687\n",
      "[38]\ttrain-mlogloss:1.09035\tdval-mlogloss:1.33681\n",
      "[39]\ttrain-mlogloss:1.07936\tdval-mlogloss:1.3281\n",
      "[40]\ttrain-mlogloss:1.06899\tdval-mlogloss:1.31991\n",
      "[41]\ttrain-mlogloss:1.05892\tdval-mlogloss:1.31106\n",
      "[42]\ttrain-mlogloss:1.04912\tdval-mlogloss:1.30323\n",
      "[43]\ttrain-mlogloss:1.03976\tdval-mlogloss:1.29541\n",
      "[44]\ttrain-mlogloss:1.03081\tdval-mlogloss:1.28822\n",
      "[45]\ttrain-mlogloss:1.022\tdval-mlogloss:1.28139\n",
      "[46]\ttrain-mlogloss:1.01367\tdval-mlogloss:1.27539\n",
      "[47]\ttrain-mlogloss:1.00556\tdval-mlogloss:1.26896\n",
      "[48]\ttrain-mlogloss:0.997923\tdval-mlogloss:1.26299\n",
      "[49]\ttrain-mlogloss:0.990464\tdval-mlogloss:1.25725\n",
      "[50]\ttrain-mlogloss:0.98336\tdval-mlogloss:1.25166\n",
      "[51]\ttrain-mlogloss:0.976336\tdval-mlogloss:1.24651\n",
      "[52]\ttrain-mlogloss:0.969727\tdval-mlogloss:1.24186\n",
      "[53]\ttrain-mlogloss:0.963156\tdval-mlogloss:1.23614\n",
      "[54]\ttrain-mlogloss:0.957059\tdval-mlogloss:1.23079\n",
      "[55]\ttrain-mlogloss:0.951129\tdval-mlogloss:1.22603\n",
      "[56]\ttrain-mlogloss:0.945419\tdval-mlogloss:1.22101\n",
      "[57]\ttrain-mlogloss:0.939796\tdval-mlogloss:1.21642\n",
      "[58]\ttrain-mlogloss:0.934345\tdval-mlogloss:1.21137\n",
      "[59]\ttrain-mlogloss:0.929013\tdval-mlogloss:1.20661\n",
      "[60]\ttrain-mlogloss:0.923828\tdval-mlogloss:1.20213\n",
      "[61]\ttrain-mlogloss:0.918815\tdval-mlogloss:1.19849\n",
      "[62]\ttrain-mlogloss:0.914152\tdval-mlogloss:1.19488\n",
      "[63]\ttrain-mlogloss:0.909535\tdval-mlogloss:1.19123\n",
      "[64]\ttrain-mlogloss:0.905038\tdval-mlogloss:1.18794\n",
      "[65]\ttrain-mlogloss:0.900657\tdval-mlogloss:1.18412\n",
      "[66]\ttrain-mlogloss:0.896474\tdval-mlogloss:1.1808\n",
      "[67]\ttrain-mlogloss:0.892477\tdval-mlogloss:1.17746\n",
      "[68]\ttrain-mlogloss:0.888487\tdval-mlogloss:1.17485\n",
      "[69]\ttrain-mlogloss:0.884638\tdval-mlogloss:1.17193\n",
      "[70]\ttrain-mlogloss:0.880927\tdval-mlogloss:1.16891\n",
      "[71]\ttrain-mlogloss:0.877132\tdval-mlogloss:1.16608\n",
      "[72]\ttrain-mlogloss:0.873514\tdval-mlogloss:1.1637\n",
      "[73]\ttrain-mlogloss:0.870073\tdval-mlogloss:1.16169\n",
      "[74]\ttrain-mlogloss:0.866777\tdval-mlogloss:1.1592\n",
      "[75]\ttrain-mlogloss:0.863555\tdval-mlogloss:1.15679\n",
      "[76]\ttrain-mlogloss:0.860477\tdval-mlogloss:1.15459\n",
      "[77]\ttrain-mlogloss:0.857376\tdval-mlogloss:1.15244\n",
      "[78]\ttrain-mlogloss:0.854309\tdval-mlogloss:1.15016\n",
      "[79]\ttrain-mlogloss:0.851395\tdval-mlogloss:1.14799\n",
      "[80]\ttrain-mlogloss:0.848526\tdval-mlogloss:1.14631\n",
      "[81]\ttrain-mlogloss:0.845861\tdval-mlogloss:1.14517\n",
      "[82]\ttrain-mlogloss:0.843055\tdval-mlogloss:1.14357\n",
      "[83]\ttrain-mlogloss:0.840385\tdval-mlogloss:1.14209\n",
      "[84]\ttrain-mlogloss:0.837749\tdval-mlogloss:1.14042\n",
      "[85]\ttrain-mlogloss:0.835239\tdval-mlogloss:1.13834\n",
      "[86]\ttrain-mlogloss:0.832789\tdval-mlogloss:1.13646\n",
      "[87]\ttrain-mlogloss:0.830402\tdval-mlogloss:1.13528\n",
      "[88]\ttrain-mlogloss:0.828021\tdval-mlogloss:1.13386\n",
      "[89]\ttrain-mlogloss:0.825699\tdval-mlogloss:1.13206\n",
      "[90]\ttrain-mlogloss:0.823346\tdval-mlogloss:1.13104\n",
      "[91]\ttrain-mlogloss:0.821084\tdval-mlogloss:1.12945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3e41ac1d7117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mdtrain_june15\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_june15\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_june15\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdval_june15\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val_june15\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_june15\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel_june15\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain_june15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain_june15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdval_june15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dval'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mpreds_june15\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_june15\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_june15\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m--> 894\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m    895\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train_june15, y_train_june15 = create_train('2015-06-28', pattern_flag=True)\n",
    "x_val_june15, y_val_june15 = create_train('2016-05-28', pattern_flag=True)\n",
    "\n",
    "x_test_june15 = create_test(pattern_flag=True)\n",
    "\n",
    "param = {'objective': 'multi:softprob', \n",
    "         'eta': 0.05, \n",
    "         'max_depth': 8, \n",
    "         'silent': 1, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'mlogloss',\n",
    "         'min_child_weight': 1,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 0}\n",
    "num_rounds = 100\n",
    "\n",
    "dtrain_june15 = xgb.DMatrix(x_train_june15.values, y_train_june15.values)\n",
    "dval_june15 = xgb.DMatrix(x_val_june15.values, y_val_june15.values)\n",
    "model_june15 = xgb.train(param, dtrain_june15, num_rounds, evals=[(dtrain_june15, 'train'), (dval_june15, 'dval')], verbose_eval=True)\n",
    "\n",
    "preds_june15 = model_june15.predict(xgb.DMatrix(x_test_june15.values))\n",
    "\n",
    "df_preds_june15 = pd.DataFrame(preds_june15, index=x_test_june15.index, columns=target_cols)\n",
    "# Remove already bought products \n",
    "df_preds_june15[x_test_june15[target_cols]==1] = 0 \n",
    "preds_june15 = df_preds_june15.values\n",
    "preds_june15 = np.argsort(preds_june15, axis=1)\n",
    "preds_june15 = np.fliplr(preds_june15)[:, :7]\n",
    "\n",
    "test_id = x_test_june15.loc[:, 'ncodpers'].values\n",
    "final_preds_june15 = [' '.join([target_cols[k] for k in pred]) for pred in preds_june15]\n",
    "\n",
    "out_df_june15 = pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds_june15})\n",
    "out_df_june15.to_csv('eda_4_28_june15.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-05-28, max_lag=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.64321\n",
      "[1]\ttrain-mlogloss:2.42916\n",
      "[2]\ttrain-mlogloss:2.26143\n",
      "[3]\ttrain-mlogloss:2.12057\n",
      "[4]\ttrain-mlogloss:2.00099\n",
      "[5]\ttrain-mlogloss:1.89603\n",
      "[6]\ttrain-mlogloss:1.80356\n",
      "[7]\ttrain-mlogloss:1.72067\n",
      "[8]\ttrain-mlogloss:1.64465\n",
      "[9]\ttrain-mlogloss:1.57663\n",
      "[10]\ttrain-mlogloss:1.51355\n",
      "[11]\ttrain-mlogloss:1.45557\n",
      "[12]\ttrain-mlogloss:1.40214\n",
      "[13]\ttrain-mlogloss:1.35231\n",
      "[14]\ttrain-mlogloss:1.30587\n",
      "[15]\ttrain-mlogloss:1.26249\n",
      "[16]\ttrain-mlogloss:1.222\n",
      "[17]\ttrain-mlogloss:1.18383\n",
      "[18]\ttrain-mlogloss:1.14811\n",
      "[19]\ttrain-mlogloss:1.11448\n",
      "[20]\ttrain-mlogloss:1.08268\n",
      "[21]\ttrain-mlogloss:1.0525\n",
      "[22]\ttrain-mlogloss:1.02404\n",
      "[23]\ttrain-mlogloss:0.997108\n",
      "[24]\ttrain-mlogloss:0.971606\n",
      "[25]\ttrain-mlogloss:0.947348\n",
      "[26]\ttrain-mlogloss:0.924398\n",
      "[27]\ttrain-mlogloss:0.902768\n",
      "[28]\ttrain-mlogloss:0.881792\n",
      "[29]\ttrain-mlogloss:0.861978\n",
      "[30]\ttrain-mlogloss:0.842969\n",
      "[31]\ttrain-mlogloss:0.824979\n",
      "[32]\ttrain-mlogloss:0.80799\n",
      "[33]\ttrain-mlogloss:0.791558\n",
      "[34]\ttrain-mlogloss:0.776193\n",
      "[35]\ttrain-mlogloss:0.761276\n",
      "[36]\ttrain-mlogloss:0.746991\n",
      "[37]\ttrain-mlogloss:0.733397\n",
      "[38]\ttrain-mlogloss:0.720404\n",
      "[39]\ttrain-mlogloss:0.707905\n",
      "[40]\ttrain-mlogloss:0.696077\n",
      "[41]\ttrain-mlogloss:0.684611\n",
      "[42]\ttrain-mlogloss:0.673712\n",
      "[43]\ttrain-mlogloss:0.66345\n",
      "[44]\ttrain-mlogloss:0.653513\n",
      "[45]\ttrain-mlogloss:0.643782\n",
      "[46]\ttrain-mlogloss:0.634649\n",
      "[47]\ttrain-mlogloss:0.625692\n",
      "[48]\ttrain-mlogloss:0.617063\n",
      "[49]\ttrain-mlogloss:0.608998\n",
      "[50]\ttrain-mlogloss:0.60114\n",
      "[51]\ttrain-mlogloss:0.593598\n",
      "[52]\ttrain-mlogloss:0.586282\n",
      "[53]\ttrain-mlogloss:0.57932\n",
      "[54]\ttrain-mlogloss:0.57262\n",
      "[55]\ttrain-mlogloss:0.566129\n",
      "[56]\ttrain-mlogloss:0.560005\n",
      "[57]\ttrain-mlogloss:0.554003\n",
      "[58]\ttrain-mlogloss:0.548272\n",
      "[59]\ttrain-mlogloss:0.542817\n"
     ]
    }
   ],
   "source": [
    "x_train_may16, y_train_may16 = create_train('2015-05-28', pattern_flag=True, max_lag=16)\n",
    "\n",
    "x_test_may16 = create_test(pattern_flag=True)\n",
    "\n",
    "param = {'objective': 'multi:softprob', \n",
    "         'eta': 0.05, \n",
    "         'max_depth': 8, \n",
    "         'silent': 1, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'mlogloss',\n",
    "         'min_child_weight': 1,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 0}\n",
    "num_rounds = 60\n",
    "\n",
    "dtrain_may16 = xgb.DMatrix(x_train_may16.values, y_train_may16.values)\n",
    "model_may16 = xgb.train(param, dtrain_may16, num_rounds, evals=[(dtrain_may16, 'train')], verbose_eval=True)\n",
    "\n",
    "preds_may16 = model_may16.predict(xgb.DMatrix(x_test_may16.values))\n",
    "\n",
    "df_preds_may16 = pd.DataFrame(preds_may16, index=x_test_may16.index, columns=target_cols)\n",
    "# Remove already bought products \n",
    "df_preds_may16[x_test_may16[target_cols]==1] = 0 \n",
    "preds_may16 = df_preds_may16.values\n",
    "preds_may16 = np.argsort(preds_may16, axis=1)\n",
    "preds_may16 = np.fliplr(preds_may16)[:, :7]\n",
    "\n",
    "test_id = x_test_may16.loc[:, 'ncodpers'].values\n",
    "final_preds_may16 = [' '.join([target_cols[k] for k in pred]) for pred in preds_may16]\n",
    "\n",
    "out_df_may16 = pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds_may16})\n",
    "out_df_may16.to_csv('eda_4_28_may16.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_june15 = model_june15.predict(xgb.DMatrix(x_test_june15.values))\n",
    "preds_may16 = model_may16.predict(xgb.DMatrix(x_test_may16.values))\n",
    "\n",
    "preds1 = np.sqrt(preds_june15*preds_may16)\n",
    "preds2 = 0.5*preds_june15 + 0.5*preds_may16\n",
    "\n",
    "# Geometry mean\n",
    "df_preds1 = pd.DataFrame(preds1, index=x_test_may16.index, columns=target_cols)\n",
    "# Remove already bought products \n",
    "df_preds1[x_test_may16[target_cols]==1] = 0 \n",
    "preds1 = df_preds1.values\n",
    "preds1 = np.argsort(preds1, axis=1)\n",
    "preds1 = np.fliplr(preds1)[:, :7]\n",
    "\n",
    "test_id = x_test_may16.loc[:, 'ncodpers'].values\n",
    "final_preds1 = [' '.join([target_cols[k] for k in pred]) for pred in preds1]\n",
    "\n",
    "out_df1 = pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds1})\n",
    "out_df1.to_csv('eda_4_28_gm.csv.gz', compression='gzip', index=False)\n",
    "\n",
    "# Algorithmic mean\n",
    "df_preds2 = pd.DataFrame(preds2, index=x_test_may16.index, columns=target_cols)\n",
    "# Remove already bought products \n",
    "df_preds2[x_test_may16[target_cols]==1] = 0 \n",
    "preds2 = df_preds2.values\n",
    "preds2 = np.argsort(preds2, axis=1)\n",
    "preds2 = np.fliplr(preds2)[:, :7]\n",
    "\n",
    "test_id = x_test_may16.loc[:, 'ncodpers'].values\n",
    "final_preds2 = [' '.join([target_cols[k] for k in pred]) for pred in preds2]\n",
    "\n",
    "out_df2 = pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds2})\n",
    "out_df2.to_csv('eda_4_28_am.csv.gz', compression='gzip', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
