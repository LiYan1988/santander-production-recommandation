{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping\n",
    "\n",
    "Clean code, write functions to generate all necessary preprocessing and data.\n",
    "\n",
    "Data Preparation\n",
    "1. `create_train`\n",
    "2. `calculate_customer_product_pair`\n",
    "3. generate `data_month_{}.hdf`\n",
    "    - `eda_4_7.ipynb` to `eda_4_10.ipynb`\n",
    "4. `calculate_customer_product_pair`\n",
    "5. `mean_encoding_month_product`\n",
    "6. `count_pattern_2`\n",
    "7. `count_history` and its helper functions\n",
    "8. `calculate_weight`\n",
    "9. `train_test_month`\n",
    "10. calculate feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from santander_helper import *\n",
    "%matplotlib inline\n",
    "\n",
    "create_monthly_data()\n",
    "\n",
    "target = calculate_customer_product_pair()\n",
    "\n",
    "mean_encoding_result = mean_encoding_month_product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "# param = {'objective': 'multi:softprob', \n",
    "#          'eta': 0.05, \n",
    "#          'max_depth': 12, \n",
    "#          'silent': 1, \n",
    "#          'num_class': len(target_cols),\n",
    "#          'eval_metric': 'mlogloss',\n",
    "#          'min_child_weight': 1,\n",
    "#          'subsample': 0.7,\n",
    "#          'colsample_bytree': 0.7,\n",
    "#          'seed': 0}\n",
    "\n",
    "# fi = calculate_feature_importance(param, \n",
    "#                                   num_rounds=50, \n",
    "#                                   n_repeat=5,\n",
    "#                                   random_seed=42,\n",
    "#                                   fi_name='feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on 2015-06-28 and predict\n",
    "simulation_name0 = 'eda_5_1'\n",
    "simulation_name1 = '1506_1'\n",
    "param = {'objective': 'multi:softprob', \n",
    "         'eta': 0.05, \n",
    "         'max_depth': 8,\n",
    "         'silent': 1, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'merror',\n",
    "         'min_child_weight': 10,\n",
    "         'min_split_loss': 1,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 3870}\n",
    "num_rounds = 125\n",
    "n_repeat = 5\n",
    "\n",
    "history, model_dict, y_pred, y_sub = \\\n",
    "    train_test_month(param, num_rounds, '2015-06-28', '2016-05-28', \n",
    "    sub_name='{}_{}.csv.gz'.format(simulation_name0, simulation_name1), \n",
    "    n_repeat=n_repeat, random_seed=54, \n",
    "    n_features=350, eval_train_flag=False)\n",
    "\n",
    "# History and learning curve\n",
    "plot_history_val(history)\n",
    "\n",
    "# Feature importance\n",
    "fi = plot_feature_importance(model_dict)\n",
    "\n",
    "# Save data\n",
    "save_pickle('{}_{}.pickle'.format(simulation_name0, simulation_name1), (history, model_dict, y_pred, y_sub, fi, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all months' data\n",
    "x_train = []\n",
    "y_train = []\n",
    "w_train = []\n",
    "fixed_lag = 6\n",
    "for i, m in tqdm.tqdm_notebook(enumerate(month_list), total=len(month_list)):\n",
    "    if m in ['2015-01-28', '2016-06-28']:\n",
    "        continue\n",
    "    x_tmp, y_tmp, w_tmp = create_train(m, max_lag=i, fixed_lag=fixed_lag, pattern_flag=True)\n",
    "    x_train.append(x_tmp)\n",
    "    y_train.append(y_tmp)\n",
    "    w_train.append(w_tmp)\n",
    "del x_tmp, y_tmp, w_tmp\n",
    "gc.collect()\n",
    "\n",
    "x_train = pd.concat(x_train, axis=0, ignore_index=True, sort=False)\n",
    "y_train = pd.concat(y_train, axis=0, ignore_index=True, sort=False)\n",
    "w_train = pd.concat(w_train, axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "param = {'objective': 'multi:softprob', \n",
    "         'eta': 0.1, \n",
    "         'max_depth': 8,\n",
    "         'silent': 1, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'merror',\n",
    "         'min_child_weight': 10,\n",
    "         'lambda': 5,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 0}\n",
    "\n",
    "# number of rows in train dataset, to simplify testing, always set to None\n",
    "n_rows = None \n",
    "n_repeats = 3\n",
    "n_trees = 50\n",
    "train = {'x': x_train.iloc[:n_rows, :], 'y': y_train.iloc[:n_rows], 'w': w_train.iloc[:n_rows]}\n",
    "clfs, running_time = cv_all_month(param, train, n_features=350, num_boost_round=n_trees, \n",
    "    n_repeats=n_repeats, random_state=3870, verbose_eval=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
