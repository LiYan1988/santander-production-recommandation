{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and CV based on 5-th Place Solutions\n",
    "\n",
    "`param = {'objective': 'multi:softprob', \n",
    "         'eta': 0.05, \n",
    "         'max_depth': 8,\n",
    "         'silent': 1, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'mlogloss',\n",
    "         'min_child_weight': 1,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 0}`\n",
    "         \n",
    "`n_repeats=2\n",
    "n_trees = 150`\n",
    "\n",
    "val-MAP@7:0.89935, private LB: 0.0266884, public LB: 0.0264044\n",
    "\n",
    "New in this notebook:\n",
    "- Create training data for all months (2015-02-28 to 2016-04-28) and validate on 2016-05-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from santander_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all months' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fixed_lag = 6\n",
    "# for i, m in tqdm.tqdm_notebook(enumerate(month_list), total=len(month_list)):\n",
    "#     print(m)\n",
    "#     if m in ['2015-01-28', '2016-06-28']:\n",
    "#         continue\n",
    "#     x_train, y_train, w_train = create_train(m, max_lag=i, fixed_lag=fixed_lag, pattern_flag=True)\n",
    "#     print('-'*60)\n",
    "# del x_train, y_train, w_train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all months' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a82711fd2204afb8b9ed970d8182e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "w_train = []\n",
    "fixed_lag = 6\n",
    "for i, m in tqdm.tqdm_notebook(enumerate(month_list), total=len(month_list)):\n",
    "    if m in ['2015-01-28', '2016-06-28']:\n",
    "        continue\n",
    "    x_tmp, y_tmp, w_tmp = create_train(m, max_lag=i, fixed_lag=fixed_lag, pattern_flag=True)\n",
    "    x_train.append(x_tmp)\n",
    "    y_train.append(y_tmp)\n",
    "    w_train.append(w_tmp)\n",
    "del x_tmp, y_tmp, w_tmp\n",
    "gc.collect()\n",
    "\n",
    "# Prepare for train and validation\n",
    "x_val = x_train[-1]\n",
    "y_val = y_train[-1]\n",
    "w_val = w_train[-1]\n",
    "\n",
    "x_train = pd.concat(x_train[:-1], axis=0, ignore_index=True, sort=False)\n",
    "y_train = pd.concat(y_train[:-1], axis=0, ignore_index=True, sort=False)\n",
    "w_train = pd.concat(w_train[:-1], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.66292\tval-mlogloss:2.67474\ttrain-MAP@7:0\tval-MAP@7:0.893116\n",
      "[1]\ttrain-mlogloss:2.46916\tval-mlogloss:2.486\ttrain-MAP@7:0\tval-MAP@7:0.897498\n",
      "[2]\ttrain-mlogloss:2.31823\tval-mlogloss:2.33769\ttrain-MAP@7:0\tval-MAP@7:0.898923\n",
      "[3]\ttrain-mlogloss:2.19381\tval-mlogloss:2.21584\ttrain-MAP@7:0\tval-MAP@7:0.899542\n",
      "[4]\ttrain-mlogloss:2.08748\tval-mlogloss:2.11122\ttrain-MAP@7:0\tval-MAP@7:0.900673\n",
      "[5]\ttrain-mlogloss:1.99476\tval-mlogloss:2.01956\ttrain-MAP@7:0\tval-MAP@7:0.901178\n",
      "[6]\ttrain-mlogloss:1.91287\tval-mlogloss:1.93849\ttrain-MAP@7:0\tval-MAP@7:0.901897\n",
      "[7]\ttrain-mlogloss:1.83969\tval-mlogloss:1.8672\ttrain-MAP@7:0\tval-MAP@7:0.902293\n",
      "[8]\ttrain-mlogloss:1.77395\tval-mlogloss:1.80178\ttrain-MAP@7:0\tval-MAP@7:0.902894\n",
      "[9]\ttrain-mlogloss:1.7143\tval-mlogloss:1.7428\ttrain-MAP@7:0\tval-MAP@7:0.903168\n",
      "[10]\ttrain-mlogloss:1.65943\tval-mlogloss:1.68941\ttrain-MAP@7:0\tval-MAP@7:0.903383\n",
      "[11]\ttrain-mlogloss:1.60891\tval-mlogloss:1.63917\ttrain-MAP@7:0\tval-MAP@7:0.903694\n",
      "[12]\ttrain-mlogloss:1.56214\tval-mlogloss:1.59353\ttrain-MAP@7:0\tval-MAP@7:0.904006\n",
      "[13]\ttrain-mlogloss:1.51879\tval-mlogloss:1.55073\ttrain-MAP@7:0\tval-MAP@7:0.904162\n",
      "[14]\ttrain-mlogloss:1.47851\tval-mlogloss:1.51039\ttrain-MAP@7:0\tval-MAP@7:0.904017\n",
      "[15]\ttrain-mlogloss:1.44103\tval-mlogloss:1.47278\ttrain-MAP@7:0\tval-MAP@7:0.904446\n",
      "[16]\ttrain-mlogloss:1.40585\tval-mlogloss:1.43771\ttrain-MAP@7:0\tval-MAP@7:0.904806\n",
      "[17]\ttrain-mlogloss:1.37281\tval-mlogloss:1.40464\ttrain-MAP@7:0\tval-MAP@7:0.904972\n",
      "[18]\ttrain-mlogloss:1.34212\tval-mlogloss:1.37394\ttrain-MAP@7:0\tval-MAP@7:0.905053\n",
      "[19]\ttrain-mlogloss:1.31312\tval-mlogloss:1.34492\ttrain-MAP@7:0\tval-MAP@7:0.905247\n",
      "[20]\ttrain-mlogloss:1.28582\tval-mlogloss:1.3176\ttrain-MAP@7:0\tval-MAP@7:0.905398\n",
      "[21]\ttrain-mlogloss:1.26011\tval-mlogloss:1.29245\ttrain-MAP@7:0\tval-MAP@7:0.90535\n",
      "[22]\ttrain-mlogloss:1.23595\tval-mlogloss:1.26822\ttrain-MAP@7:0\tval-MAP@7:0.905457\n",
      "[23]\ttrain-mlogloss:1.21311\tval-mlogloss:1.24555\ttrain-MAP@7:0\tval-MAP@7:0.905666\n",
      "[24]\ttrain-mlogloss:1.19132\tval-mlogloss:1.22429\ttrain-MAP@7:0\tval-MAP@7:0.905762\n",
      "[25]\ttrain-mlogloss:1.17082\tval-mlogloss:1.2037\ttrain-MAP@7:0\tval-MAP@7:0.905924\n",
      "[26]\ttrain-mlogloss:1.15144\tval-mlogloss:1.18424\ttrain-MAP@7:0\tval-MAP@7:0.90592\n",
      "[27]\ttrain-mlogloss:1.13288\tval-mlogloss:1.16557\ttrain-MAP@7:0\tval-MAP@7:0.905924\n",
      "[28]\ttrain-mlogloss:1.11529\tval-mlogloss:1.14774\ttrain-MAP@7:0\tval-MAP@7:0.906052\n",
      "[29]\ttrain-mlogloss:1.09857\tval-mlogloss:1.13098\ttrain-MAP@7:0\tval-MAP@7:0.906333\n",
      "[30]\ttrain-mlogloss:1.08281\tval-mlogloss:1.11513\ttrain-MAP@7:0\tval-MAP@7:0.906376\n",
      "[31]\ttrain-mlogloss:1.06787\tval-mlogloss:1.10006\ttrain-MAP@7:0\tval-MAP@7:0.906516\n",
      "[32]\ttrain-mlogloss:1.05349\tval-mlogloss:1.08561\ttrain-MAP@7:0\tval-MAP@7:0.906621\n",
      "[33]\ttrain-mlogloss:1.03998\tval-mlogloss:1.07195\ttrain-MAP@7:0\tval-MAP@7:0.906618\n",
      "[34]\ttrain-mlogloss:1.02699\tval-mlogloss:1.05904\ttrain-MAP@7:0\tval-MAP@7:0.906737\n",
      "[35]\ttrain-mlogloss:1.01451\tval-mlogloss:1.04647\ttrain-MAP@7:0\tval-MAP@7:0.906808\n",
      "[36]\ttrain-mlogloss:1.00264\tval-mlogloss:1.03457\ttrain-MAP@7:0\tval-MAP@7:0.906968\n",
      "[37]\ttrain-mlogloss:0.991324\tval-mlogloss:1.02305\ttrain-MAP@7:0\tval-MAP@7:0.907012\n",
      "[38]\ttrain-mlogloss:0.980644\tval-mlogloss:1.01235\ttrain-MAP@7:0\tval-MAP@7:0.9071\n",
      "[39]\ttrain-mlogloss:0.970364\tval-mlogloss:1.00233\ttrain-MAP@7:0\tval-MAP@7:0.907213\n",
      "[40]\ttrain-mlogloss:0.960575\tval-mlogloss:0.992391\ttrain-MAP@7:0\tval-MAP@7:0.907362\n",
      "[41]\ttrain-mlogloss:0.95128\tval-mlogloss:0.982857\ttrain-MAP@7:0\tval-MAP@7:0.907497\n",
      "[42]\ttrain-mlogloss:0.94233\tval-mlogloss:0.973696\ttrain-MAP@7:0\tval-MAP@7:0.907607\n",
      "[43]\ttrain-mlogloss:0.933755\tval-mlogloss:0.964934\ttrain-MAP@7:0\tval-MAP@7:0.907778\n",
      "[44]\ttrain-mlogloss:0.925665\tval-mlogloss:0.956651\ttrain-MAP@7:0\tval-MAP@7:0.907883\n",
      "[45]\ttrain-mlogloss:0.917901\tval-mlogloss:0.948654\ttrain-MAP@7:0\tval-MAP@7:0.908003\n",
      "[46]\ttrain-mlogloss:0.910396\tval-mlogloss:0.941046\ttrain-MAP@7:0\tval-MAP@7:0.908068\n",
      "[47]\ttrain-mlogloss:0.903202\tval-mlogloss:0.933795\ttrain-MAP@7:0\tval-MAP@7:0.907988\n",
      "[48]\ttrain-mlogloss:0.89628\tval-mlogloss:0.926744\ttrain-MAP@7:0\tval-MAP@7:0.90822\n",
      "[49]\ttrain-mlogloss:0.889667\tval-mlogloss:0.920162\ttrain-MAP@7:0\tval-MAP@7:0.9082\n",
      "[50]\ttrain-mlogloss:0.883391\tval-mlogloss:0.913915\ttrain-MAP@7:0\tval-MAP@7:0.908289\n",
      "[51]\ttrain-mlogloss:0.877363\tval-mlogloss:0.907783\ttrain-MAP@7:0\tval-MAP@7:0.908326\n",
      "[52]\ttrain-mlogloss:0.871573\tval-mlogloss:0.901691\ttrain-MAP@7:0\tval-MAP@7:0.908432\n",
      "[53]\ttrain-mlogloss:0.865958\tval-mlogloss:0.896072\ttrain-MAP@7:0\tval-MAP@7:0.908533\n",
      "[54]\ttrain-mlogloss:0.86065\tval-mlogloss:0.890614\ttrain-MAP@7:0\tval-MAP@7:0.908518\n",
      "[55]\ttrain-mlogloss:0.855488\tval-mlogloss:0.885306\ttrain-MAP@7:0\tval-MAP@7:0.908623\n",
      "[56]\ttrain-mlogloss:0.850492\tval-mlogloss:0.880181\ttrain-MAP@7:0\tval-MAP@7:0.908755\n",
      "[57]\ttrain-mlogloss:0.8457\tval-mlogloss:0.875284\ttrain-MAP@7:0\tval-MAP@7:0.908783\n",
      "[58]\ttrain-mlogloss:0.841154\tval-mlogloss:0.870564\ttrain-MAP@7:0\tval-MAP@7:0.90873\n",
      "[59]\ttrain-mlogloss:0.83678\tval-mlogloss:0.866048\ttrain-MAP@7:0\tval-MAP@7:0.908771\n",
      "[60]\ttrain-mlogloss:0.832538\tval-mlogloss:0.861782\ttrain-MAP@7:0\tval-MAP@7:0.908775\n",
      "[61]\ttrain-mlogloss:0.828525\tval-mlogloss:0.857533\ttrain-MAP@7:0\tval-MAP@7:0.908882\n",
      "[62]\ttrain-mlogloss:0.824596\tval-mlogloss:0.853569\ttrain-MAP@7:0\tval-MAP@7:0.90896\n",
      "[63]\ttrain-mlogloss:0.820878\tval-mlogloss:0.849689\ttrain-MAP@7:0\tval-MAP@7:0.909135\n",
      "[64]\ttrain-mlogloss:0.817245\tval-mlogloss:0.845958\ttrain-MAP@7:0\tval-MAP@7:0.908985\n",
      "[65]\ttrain-mlogloss:0.813714\tval-mlogloss:0.842321\ttrain-MAP@7:0\tval-MAP@7:0.90895\n",
      "[66]\ttrain-mlogloss:0.810355\tval-mlogloss:0.838859\ttrain-MAP@7:0\tval-MAP@7:0.909079\n",
      "[67]\ttrain-mlogloss:0.807077\tval-mlogloss:0.835436\ttrain-MAP@7:0\tval-MAP@7:0.909257\n",
      "[68]\ttrain-mlogloss:0.803929\tval-mlogloss:0.83223\ttrain-MAP@7:0\tval-MAP@7:0.909313\n",
      "[69]\ttrain-mlogloss:0.800894\tval-mlogloss:0.829138\ttrain-MAP@7:0\tval-MAP@7:0.909379\n",
      "[70]\ttrain-mlogloss:0.798064\tval-mlogloss:0.826358\ttrain-MAP@7:0\tval-MAP@7:0.909361\n",
      "[71]\ttrain-mlogloss:0.795222\tval-mlogloss:0.82343\ttrain-MAP@7:0\tval-MAP@7:0.909355\n",
      "[72]\ttrain-mlogloss:0.792471\tval-mlogloss:0.820656\ttrain-MAP@7:0\tval-MAP@7:0.909413\n",
      "[73]\ttrain-mlogloss:0.789876\tval-mlogloss:0.818069\ttrain-MAP@7:0\tval-MAP@7:0.909417\n",
      "[74]\ttrain-mlogloss:0.787323\tval-mlogloss:0.815347\ttrain-MAP@7:0\tval-MAP@7:0.909518\n",
      "[75]\ttrain-mlogloss:0.784863\tval-mlogloss:0.812895\ttrain-MAP@7:0\tval-MAP@7:0.909527\n",
      "[76]\ttrain-mlogloss:0.782412\tval-mlogloss:0.810396\ttrain-MAP@7:0\tval-MAP@7:0.909728\n",
      "[77]\ttrain-mlogloss:0.780091\tval-mlogloss:0.807941\ttrain-MAP@7:0\tval-MAP@7:0.909656\n",
      "[78]\ttrain-mlogloss:0.777858\tval-mlogloss:0.805679\ttrain-MAP@7:0\tval-MAP@7:0.909715\n",
      "[79]\ttrain-mlogloss:0.775705\tval-mlogloss:0.803507\ttrain-MAP@7:0\tval-MAP@7:0.909808\n",
      "[80]\ttrain-mlogloss:0.773611\tval-mlogloss:0.801496\ttrain-MAP@7:0\tval-MAP@7:0.909852\n",
      "[81]\ttrain-mlogloss:0.771581\tval-mlogloss:0.799478\ttrain-MAP@7:0\tval-MAP@7:0.909862\n",
      "[82]\ttrain-mlogloss:0.769628\tval-mlogloss:0.79741\ttrain-MAP@7:0\tval-MAP@7:0.909971\n",
      "[83]\ttrain-mlogloss:0.767703\tval-mlogloss:0.79544\ttrain-MAP@7:0\tval-MAP@7:0.910049\n",
      "[84]\ttrain-mlogloss:0.765849\tval-mlogloss:0.793429\ttrain-MAP@7:0\tval-MAP@7:0.910161\n",
      "[85]\ttrain-mlogloss:0.764033\tval-mlogloss:0.791585\ttrain-MAP@7:0\tval-MAP@7:0.910105\n",
      "[86]\ttrain-mlogloss:0.762315\tval-mlogloss:0.789702\ttrain-MAP@7:0\tval-MAP@7:0.910066\n",
      "[87]\ttrain-mlogloss:0.760611\tval-mlogloss:0.788002\ttrain-MAP@7:0\tval-MAP@7:0.910095\n",
      "[88]\ttrain-mlogloss:0.758982\tval-mlogloss:0.786256\ttrain-MAP@7:0\tval-MAP@7:0.910193\n",
      "[89]\ttrain-mlogloss:0.757388\tval-mlogloss:0.784612\ttrain-MAP@7:0\tval-MAP@7:0.91015\n",
      "[90]\ttrain-mlogloss:0.755827\tval-mlogloss:0.783084\ttrain-MAP@7:0\tval-MAP@7:0.910214\n",
      "[91]\ttrain-mlogloss:0.754323\tval-mlogloss:0.781572\ttrain-MAP@7:0\tval-MAP@7:0.910139\n",
      "[92]\ttrain-mlogloss:0.752853\tval-mlogloss:0.780055\ttrain-MAP@7:0\tval-MAP@7:0.910263\n",
      "[93]\ttrain-mlogloss:0.751439\tval-mlogloss:0.77871\ttrain-MAP@7:0\tval-MAP@7:0.910223\n",
      "[94]\ttrain-mlogloss:0.750076\tval-mlogloss:0.777337\ttrain-MAP@7:0\tval-MAP@7:0.910291\n",
      "[95]\ttrain-mlogloss:0.748761\tval-mlogloss:0.775949\ttrain-MAP@7:0\tval-MAP@7:0.910409\n",
      "[96]\ttrain-mlogloss:0.747459\tval-mlogloss:0.774616\ttrain-MAP@7:0\tval-MAP@7:0.910423\n",
      "[97]\ttrain-mlogloss:0.746173\tval-mlogloss:0.773409\ttrain-MAP@7:0\tval-MAP@7:0.910444\n",
      "[98]\ttrain-mlogloss:0.744941\tval-mlogloss:0.772228\ttrain-MAP@7:0\tval-MAP@7:0.910351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99]\ttrain-mlogloss:0.74372\tval-mlogloss:0.771048\ttrain-MAP@7:0\tval-MAP@7:0.910264\n",
      "[100]\ttrain-mlogloss:0.742555\tval-mlogloss:0.769932\ttrain-MAP@7:0\tval-MAP@7:0.910319\n",
      "[101]\ttrain-mlogloss:0.741441\tval-mlogloss:0.76881\ttrain-MAP@7:0\tval-MAP@7:0.910422\n",
      "[102]\ttrain-mlogloss:0.740364\tval-mlogloss:0.767745\ttrain-MAP@7:0\tval-MAP@7:0.910517\n",
      "[103]\ttrain-mlogloss:0.739239\tval-mlogloss:0.766603\ttrain-MAP@7:0\tval-MAP@7:0.910583\n",
      "[104]\ttrain-mlogloss:0.738224\tval-mlogloss:0.76562\ttrain-MAP@7:0\tval-MAP@7:0.910527\n",
      "[105]\ttrain-mlogloss:0.73721\tval-mlogloss:0.764652\ttrain-MAP@7:0\tval-MAP@7:0.910671\n",
      "[106]\ttrain-mlogloss:0.736178\tval-mlogloss:0.76371\ttrain-MAP@7:0\tval-MAP@7:0.910706\n",
      "[107]\ttrain-mlogloss:0.735202\tval-mlogloss:0.762874\ttrain-MAP@7:0\tval-MAP@7:0.910689\n",
      "[108]\ttrain-mlogloss:0.734225\tval-mlogloss:0.761904\ttrain-MAP@7:0\tval-MAP@7:0.910715\n",
      "[109]\ttrain-mlogloss:0.733256\tval-mlogloss:0.76098\ttrain-MAP@7:0\tval-MAP@7:0.910757\n",
      "[110]\ttrain-mlogloss:0.732339\tval-mlogloss:0.760014\ttrain-MAP@7:0\tval-MAP@7:0.910813\n",
      "[111]\ttrain-mlogloss:0.731431\tval-mlogloss:0.759256\ttrain-MAP@7:0\tval-MAP@7:0.910821\n",
      "[112]\ttrain-mlogloss:0.730584\tval-mlogloss:0.75847\ttrain-MAP@7:0\tval-MAP@7:0.910816\n",
      "[113]\ttrain-mlogloss:0.729765\tval-mlogloss:0.757692\ttrain-MAP@7:0\tval-MAP@7:0.91095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c547d5128f82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mw_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m df, clfs, running_time = cv_all_month(param, train, val, n_features=350, num_boost_round=n_trees, n_splits=2, \n\u001b[1;32m---> 19\u001b[1;33m     n_repeats=n_repeats, random_state=0, verbose_eval=True)\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\santander-production-recommandation\\notebooks\\santander_helper.py\u001b[0m in \u001b[0;36mcv_all_month\u001b[1;34m(params, train, val, n_features, num_boost_round, n_splits, n_repeats, random_state, verbose_eval)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m             gt=ground_truth, ts=data_len)\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m         \u001b[0mrunning_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates, **kwargs_eval)\u001b[0m\n\u001b[0;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks, **kwargs_eval)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, **kwargs_eval)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m--> 894\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m    895\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param = {'objective': 'multi:softprob', \n",
    "         'eta': 0.1, \n",
    "         'max_depth': 8,\n",
    "         'silent': 1, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'merror',\n",
    "         'min_child_weight': 10,\n",
    "         'lambda': 5,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 0}\n",
    "\n",
    "n_rows = None # number of rows in train dataset, to simplify testing, always set to None\n",
    "n_repeats = 1\n",
    "n_trees = 50\n",
    "train = {'x': x_train.iloc[:n_rows, :], 'y': y_train.iloc[:n_rows], 'w': w_train.iloc[:n_rows]}\n",
    "val = {'x': x_val.iloc[:n_rows, :], 'y': y_val.iloc[:n_rows], 'w': w_val.iloc[:n_rows]}\n",
    "df, clfs, running_time = cv_all_month(param, train, val, n_features=350, num_boost_round=n_trees, \n",
    "    n_repeats=n_repeats, random_state=0, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = create_test(month='2016-06-28', max_lag=17, fixed_lag=6, pattern_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_name = 'p1'\n",
    "y_pred, y_sub = predict_all_month(model_dict=clfs, x_test=x_test, \n",
    "    sub_name='eda_4_33_{}.csv.gz'.format(simulation_name), n_features=350)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
