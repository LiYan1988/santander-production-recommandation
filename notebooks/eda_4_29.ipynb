{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and CV based Winners' Solutions\n",
    "\n",
    "Try to use MAP@7 as `feval` for xgboost, failed because no way to tell xgboost how to group results by users. **Can only do this after train is finished.**\n",
    "\n",
    "New in this notebook:\n",
    "- A hacky implementation of MAP@7 evaluation. \n",
    "- This method is suitable when training on one month and validate on another month, since ncodpers is the key in ground truth dictionaries.\n",
    "- This method **only works if the MAP functions and training codes are in the same notebook**.\n",
    "\n",
    "To-do: \n",
    "- mean encoding of products grouped by combinations of: canal_entrada, segmento, cod_prov\n",
    "- Time since change and lags for a few non-product features: \n",
    "    - segmento\n",
    "    - ind_actividad_cliente\n",
    "    - cod_prov\n",
    "    - canal_entrada\n",
    "    - indrel_1mes\n",
    "    - tiprel_1mes\n",
    "\n",
    "\n",
    "Features:\n",
    "- before eda_4_29\n",
    "    - average of products for each (customer, product) pair\n",
    "    - exponent weighted average of products each (customer, product) pair\n",
    "    - time since presence of products, distance to the first 1\n",
    "    - time to the last positive flank (01)\n",
    "    - time to the last negative flank (10)\n",
    "    - time to the last 1, to the nearest product purchase\n",
    "    - time to the first 1, to the first product purchase\n",
    "    - Trained@2015-06-28, validated@2015-12-28, mlogloss=1.28481\n",
    "    - Private score: 0.0302054, public score: 0.0298683\n",
    "- before eda_4_25\n",
    "    - customer info in the second month\n",
    "    - products in the first month\n",
    "    - combination of first and second month `ind_actividad_cliente`\n",
    "    - combination of first and second month `tiprel_1mes`\n",
    "    - combination of first month product by using binary number (`target_combine`)\n",
    "    - encoding `target_combine` with \n",
    "        - mean number of new products\n",
    "        - mean number of customers with new products\n",
    "        - mean number of customers with each new products\n",
    "    - Count patterns in the last `max_lag` months\n",
    "    - Number of month to the last time the customer purchase each product\n",
    "        - CV@2015-12-28: mlogloss=1.29349\n",
    "        - Private score: 0.0302475, public score: 0.0299266\n",
    "- eda_4_25\n",
    "    - Use all available history data\n",
    "        - E.g., for 2016-05-28 train data, use all previous months, for 2015-02-28, use 1 lag month. \n",
    "        - Need to create test set that use the same amount of previous months for each training data set. \n",
    "        - This is from [the second winner's solution](https://www.kaggle.com/c/santander-product-recommendation/discussion/26824), his bold part in paragraph 4.\n",
    "    - Combine models trained on 2016-05-28 and 2015-06-28:\n",
    "        - Private score: 0.0304583, public score: 0.0300839\n",
    "        - This is to catch both seasonality and trend, presented in 2015-06-28 and 2016-05-28, respectively. \n",
    "        - This idea is mentioned by many winners, like [11-th winner](https://www.kaggle.com/c/santander-product-recommendation/discussion/26823) and [14-th winner](https://www.kaggle.com/c/santander-product-recommendation/discussion/26808)\n",
    "\n",
    "- eda_4_27\n",
    "    - put 2015-06-28 and 2016-05-28 in the same data set, with the same lag=5\n",
    "        - Private score:0.0303096, public score: 0.0299867\n",
    "        - Different as [11-th winner's discussion](https://www.kaggle.com/c/santander-product-recommendation/discussion/26823)\n",
    "            > We tested this by adding 50% of May-16 data to our June model and sure enough, we went from 0.0301 to 0.0303. Then, we built separate models for Jun and May, but the ensemble didn’t work. We weren’t surprised because June data is better for seasonal products, and May data is better for trend products. And vice-versa, June data is bad for trend products and May data is bad for seasonal products. So, they sort of cancelled each other out.\n",
    "\n",
    "        - But my score is always worse than theirs, maybe this is the reason why we have different observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare two weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from santander_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.67673\tdval-mlogloss:2.69823\ttrain-MAP@7:0.869703\tdval-MAP@7:0.836259\n",
      "[1]\ttrain-mlogloss:2.49074\tdval-mlogloss:2.52495\ttrain-MAP@7:0.871119\tdval-MAP@7:0.832829\n",
      "[2]\ttrain-mlogloss:2.34356\tdval-mlogloss:2.39247\ttrain-MAP@7:0.873126\tdval-MAP@7:0.837453\n",
      "[3]\ttrain-mlogloss:2.22236\tdval-mlogloss:2.28039\ttrain-MAP@7:0.873827\tdval-MAP@7:0.837061\n",
      "[4]\ttrain-mlogloss:2.11922\tdval-mlogloss:2.18952\ttrain-MAP@7:0.875822\tdval-MAP@7:0.836688\n",
      "[5]\ttrain-mlogloss:2.02934\tdval-mlogloss:2.10505\ttrain-MAP@7:0.87617\tdval-MAP@7:0.837969\n",
      "[6]\ttrain-mlogloss:1.94954\tdval-mlogloss:2.03087\ttrain-MAP@7:0.876927\tdval-MAP@7:0.839329\n",
      "[7]\ttrain-mlogloss:1.87774\tdval-mlogloss:1.96457\ttrain-MAP@7:0.878078\tdval-MAP@7:0.840711\n",
      "[8]\ttrain-mlogloss:1.81368\tdval-mlogloss:1.90596\ttrain-MAP@7:0.877812\tdval-MAP@7:0.840521\n",
      "[9]\ttrain-mlogloss:1.75547\tdval-mlogloss:1.85178\ttrain-MAP@7:0.878262\tdval-MAP@7:0.840807\n",
      "[10]\ttrain-mlogloss:1.70167\tdval-mlogloss:1.80365\ttrain-MAP@7:0.879044\tdval-MAP@7:0.841796\n",
      "[11]\ttrain-mlogloss:1.65222\tdval-mlogloss:1.75794\ttrain-MAP@7:0.880333\tdval-MAP@7:0.843\n",
      "[12]\ttrain-mlogloss:1.60629\tdval-mlogloss:1.71694\ttrain-MAP@7:0.880683\tdval-MAP@7:0.843739\n",
      "[13]\ttrain-mlogloss:1.56353\tdval-mlogloss:1.67702\ttrain-MAP@7:0.881298\tdval-MAP@7:0.843989\n",
      "[14]\ttrain-mlogloss:1.52385\tdval-mlogloss:1.6399\ttrain-MAP@7:0.881651\tdval-MAP@7:0.84432\n",
      "[15]\ttrain-mlogloss:1.48672\tdval-mlogloss:1.60566\ttrain-MAP@7:0.881874\tdval-MAP@7:0.844414\n",
      "[16]\ttrain-mlogloss:1.45192\tdval-mlogloss:1.57349\ttrain-MAP@7:0.882084\tdval-MAP@7:0.845042\n",
      "[17]\ttrain-mlogloss:1.41942\tdval-mlogloss:1.54328\ttrain-MAP@7:0.882262\tdval-MAP@7:0.845432\n",
      "[18]\ttrain-mlogloss:1.38885\tdval-mlogloss:1.51608\ttrain-MAP@7:0.882575\tdval-MAP@7:0.845955\n",
      "[19]\ttrain-mlogloss:1.36057\tdval-mlogloss:1.49131\ttrain-MAP@7:0.882916\tdval-MAP@7:0.845728\n",
      "[20]\ttrain-mlogloss:1.33374\tdval-mlogloss:1.46699\ttrain-MAP@7:0.883067\tdval-MAP@7:0.846308\n",
      "[21]\ttrain-mlogloss:1.30839\tdval-mlogloss:1.44378\ttrain-MAP@7:0.883427\tdval-MAP@7:0.846816\n",
      "[22]\ttrain-mlogloss:1.28432\tdval-mlogloss:1.42132\ttrain-MAP@7:0.883585\tdval-MAP@7:0.846764\n",
      "[23]\ttrain-mlogloss:1.26142\tdval-mlogloss:1.40034\ttrain-MAP@7:0.884032\tdval-MAP@7:0.846397\n",
      "[24]\ttrain-mlogloss:1.23979\tdval-mlogloss:1.38149\ttrain-MAP@7:0.88425\tdval-MAP@7:0.847013\n",
      "[25]\ttrain-mlogloss:1.21973\tdval-mlogloss:1.3629\ttrain-MAP@7:0.884282\tdval-MAP@7:0.847498\n",
      "[26]\ttrain-mlogloss:1.20055\tdval-mlogloss:1.34607\ttrain-MAP@7:0.884546\tdval-MAP@7:0.847672\n",
      "[27]\ttrain-mlogloss:1.18225\tdval-mlogloss:1.32906\ttrain-MAP@7:0.88461\tdval-MAP@7:0.848007\n",
      "[28]\ttrain-mlogloss:1.16478\tdval-mlogloss:1.31405\ttrain-MAP@7:0.884779\tdval-MAP@7:0.848581\n",
      "[29]\ttrain-mlogloss:1.14812\tdval-mlogloss:1.29917\ttrain-MAP@7:0.885029\tdval-MAP@7:0.848945\n",
      "[30]\ttrain-mlogloss:1.13219\tdval-mlogloss:1.2843\ttrain-MAP@7:0.885297\tdval-MAP@7:0.849338\n",
      "[31]\ttrain-mlogloss:1.11712\tdval-mlogloss:1.2701\ttrain-MAP@7:0.885423\tdval-MAP@7:0.849428\n",
      "[32]\ttrain-mlogloss:1.10256\tdval-mlogloss:1.25663\ttrain-MAP@7:0.885466\tdval-MAP@7:0.849614\n",
      "[33]\ttrain-mlogloss:1.08875\tdval-mlogloss:1.24379\ttrain-MAP@7:0.885531\tdval-MAP@7:0.849696\n",
      "[34]\ttrain-mlogloss:1.07566\tdval-mlogloss:1.23161\ttrain-MAP@7:0.885617\tdval-MAP@7:0.849835\n",
      "[35]\ttrain-mlogloss:1.06303\tdval-mlogloss:1.21979\ttrain-MAP@7:0.8859\tdval-MAP@7:0.850028\n",
      "[36]\ttrain-mlogloss:1.05117\tdval-mlogloss:1.20893\ttrain-MAP@7:0.885991\tdval-MAP@7:0.850603\n",
      "[37]\ttrain-mlogloss:1.03983\tdval-mlogloss:1.19875\ttrain-MAP@7:0.886372\tdval-MAP@7:0.850678\n",
      "[38]\ttrain-mlogloss:1.02886\tdval-mlogloss:1.18874\ttrain-MAP@7:0.886655\tdval-MAP@7:0.850813\n",
      "[39]\ttrain-mlogloss:1.01845\tdval-mlogloss:1.17901\ttrain-MAP@7:0.886913\tdval-MAP@7:0.851174\n",
      "[40]\ttrain-mlogloss:1.00854\tdval-mlogloss:1.16986\ttrain-MAP@7:0.887054\tdval-MAP@7:0.851495\n",
      "[41]\ttrain-mlogloss:0.999056\tdval-mlogloss:1.16112\ttrain-MAP@7:0.887301\tdval-MAP@7:0.851617\n",
      "[42]\ttrain-mlogloss:0.989876\tdval-mlogloss:1.15257\ttrain-MAP@7:0.887463\tdval-MAP@7:0.851825\n",
      "[43]\ttrain-mlogloss:0.980983\tdval-mlogloss:1.14448\ttrain-MAP@7:0.887743\tdval-MAP@7:0.852108\n",
      "[44]\ttrain-mlogloss:0.972489\tdval-mlogloss:1.13756\ttrain-MAP@7:0.88782\tdval-MAP@7:0.852346\n",
      "[45]\ttrain-mlogloss:0.964227\tdval-mlogloss:1.13006\ttrain-MAP@7:0.887965\tdval-MAP@7:0.852555\n",
      "[46]\ttrain-mlogloss:0.956435\tdval-mlogloss:1.12274\ttrain-MAP@7:0.88807\tdval-MAP@7:0.85265\n",
      "[47]\ttrain-mlogloss:0.948899\tdval-mlogloss:1.11565\ttrain-MAP@7:0.888127\tdval-MAP@7:0.852713\n",
      "[48]\ttrain-mlogloss:0.941797\tdval-mlogloss:1.10913\ttrain-MAP@7:0.888289\tdval-MAP@7:0.852928\n",
      "[49]\ttrain-mlogloss:0.934893\tdval-mlogloss:1.10248\ttrain-MAP@7:0.888466\tdval-MAP@7:0.853133\n",
      "[50]\ttrain-mlogloss:0.928338\tdval-mlogloss:1.09672\ttrain-MAP@7:0.888598\tdval-MAP@7:0.853291\n",
      "[51]\ttrain-mlogloss:0.921917\tdval-mlogloss:1.09105\ttrain-MAP@7:0.888756\tdval-MAP@7:0.853315\n",
      "[52]\ttrain-mlogloss:0.915798\tdval-mlogloss:1.08564\ttrain-MAP@7:0.888823\tdval-MAP@7:0.853358\n",
      "[53]\ttrain-mlogloss:0.909829\tdval-mlogloss:1.07978\ttrain-MAP@7:0.888881\tdval-MAP@7:0.853602\n",
      "[54]\ttrain-mlogloss:0.904146\tdval-mlogloss:1.07454\ttrain-MAP@7:0.889013\tdval-MAP@7:0.853781\n",
      "[55]\ttrain-mlogloss:0.898639\tdval-mlogloss:1.06961\ttrain-MAP@7:0.889245\tdval-MAP@7:0.853926\n",
      "[56]\ttrain-mlogloss:0.893374\tdval-mlogloss:1.06467\ttrain-MAP@7:0.88933\tdval-MAP@7:0.854069\n",
      "[57]\ttrain-mlogloss:0.888308\tdval-mlogloss:1.05962\ttrain-MAP@7:0.889382\tdval-MAP@7:0.854236\n",
      "[58]\ttrain-mlogloss:0.883415\tdval-mlogloss:1.0551\ttrain-MAP@7:0.889403\tdval-MAP@7:0.854469\n",
      "[59]\ttrain-mlogloss:0.878567\tdval-mlogloss:1.05057\ttrain-MAP@7:0.889467\tdval-MAP@7:0.85478\n",
      "[60]\ttrain-mlogloss:0.873921\tdval-mlogloss:1.04604\ttrain-MAP@7:0.889613\tdval-MAP@7:0.854908\n",
      "[61]\ttrain-mlogloss:0.86946\tdval-mlogloss:1.04202\ttrain-MAP@7:0.889768\tdval-MAP@7:0.854993\n",
      "[62]\ttrain-mlogloss:0.865231\tdval-mlogloss:1.03811\ttrain-MAP@7:0.889963\tdval-MAP@7:0.855005\n",
      "[63]\ttrain-mlogloss:0.861161\tdval-mlogloss:1.03424\ttrain-MAP@7:0.890156\tdval-MAP@7:0.855226\n",
      "[64]\ttrain-mlogloss:0.857256\tdval-mlogloss:1.03102\ttrain-MAP@7:0.89032\tdval-MAP@7:0.855281\n",
      "[65]\ttrain-mlogloss:0.853385\tdval-mlogloss:1.02734\ttrain-MAP@7:0.89039\tdval-MAP@7:0.855332\n",
      "[66]\ttrain-mlogloss:0.849723\tdval-mlogloss:1.02389\ttrain-MAP@7:0.890483\tdval-MAP@7:0.855434\n",
      "[67]\ttrain-mlogloss:0.846104\tdval-mlogloss:1.02053\ttrain-MAP@7:0.890568\tdval-MAP@7:0.855648\n",
      "[68]\ttrain-mlogloss:0.842631\tdval-mlogloss:1.01755\ttrain-MAP@7:0.890584\tdval-MAP@7:0.855929\n",
      "[69]\ttrain-mlogloss:0.839211\tdval-mlogloss:1.01443\ttrain-MAP@7:0.890676\tdval-MAP@7:0.856\n",
      "[70]\ttrain-mlogloss:0.835957\tdval-mlogloss:1.01147\ttrain-MAP@7:0.89081\tdval-MAP@7:0.855993\n",
      "[71]\ttrain-mlogloss:0.83272\tdval-mlogloss:1.00894\ttrain-MAP@7:0.890927\tdval-MAP@7:0.856009\n",
      "[72]\ttrain-mlogloss:0.829548\tdval-mlogloss:1.00607\ttrain-MAP@7:0.890994\tdval-MAP@7:0.856186\n",
      "[73]\ttrain-mlogloss:0.826568\tdval-mlogloss:1.00361\ttrain-MAP@7:0.891042\tdval-MAP@7:0.856297\n",
      "[74]\ttrain-mlogloss:0.823696\tdval-mlogloss:1.00111\ttrain-MAP@7:0.891141\tdval-MAP@7:0.856377\n",
      "[75]\ttrain-mlogloss:0.820928\tdval-mlogloss:0.999045\ttrain-MAP@7:0.891273\tdval-MAP@7:0.856342\n",
      "[76]\ttrain-mlogloss:0.818286\tdval-mlogloss:0.996711\ttrain-MAP@7:0.891332\tdval-MAP@7:0.856352\n",
      "[77]\ttrain-mlogloss:0.815683\tdval-mlogloss:0.99429\ttrain-MAP@7:0.891433\tdval-MAP@7:0.856528\n",
      "[78]\ttrain-mlogloss:0.813093\tdval-mlogloss:0.991993\ttrain-MAP@7:0.891513\tdval-MAP@7:0.856629\n",
      "[79]\ttrain-mlogloss:0.810606\tdval-mlogloss:0.989735\ttrain-MAP@7:0.891657\tdval-MAP@7:0.856718\n",
      "[80]\ttrain-mlogloss:0.808167\tdval-mlogloss:0.987774\ttrain-MAP@7:0.891737\tdval-MAP@7:0.856812\n",
      "[81]\ttrain-mlogloss:0.805854\tdval-mlogloss:0.985892\ttrain-MAP@7:0.891782\tdval-MAP@7:0.856944\n",
      "[82]\ttrain-mlogloss:0.803587\tdval-mlogloss:0.98392\ttrain-MAP@7:0.891885\tdval-MAP@7:0.856974\n",
      "[83]\ttrain-mlogloss:0.801326\tdval-mlogloss:0.982512\ttrain-MAP@7:0.892009\tdval-MAP@7:0.856922\n",
      "[84]\ttrain-mlogloss:0.799156\tdval-mlogloss:0.980557\ttrain-MAP@7:0.892114\tdval-MAP@7:0.856961\n",
      "[85]\ttrain-mlogloss:0.79711\tdval-mlogloss:0.978448\ttrain-MAP@7:0.892208\tdval-MAP@7:0.857014\n",
      "[86]\ttrain-mlogloss:0.794953\tdval-mlogloss:0.976686\ttrain-MAP@7:0.89237\tdval-MAP@7:0.857178\n",
      "[87]\ttrain-mlogloss:0.792934\tdval-mlogloss:0.974995\ttrain-MAP@7:0.892487\tdval-MAP@7:0.857263\n",
      "[88]\ttrain-mlogloss:0.791034\tdval-mlogloss:0.973204\ttrain-MAP@7:0.892688\tdval-MAP@7:0.857275\n",
      "[89]\ttrain-mlogloss:0.789126\tdval-mlogloss:0.971891\ttrain-MAP@7:0.892723\tdval-MAP@7:0.85731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90]\ttrain-mlogloss:0.787226\tdval-mlogloss:0.97059\ttrain-MAP@7:0.892729\tdval-MAP@7:0.857326\n",
      "[91]\ttrain-mlogloss:0.785354\tdval-mlogloss:0.969351\ttrain-MAP@7:0.892902\tdval-MAP@7:0.857364\n",
      "[92]\ttrain-mlogloss:0.783641\tdval-mlogloss:0.96791\ttrain-MAP@7:0.893065\tdval-MAP@7:0.857372\n",
      "[93]\ttrain-mlogloss:0.78191\tdval-mlogloss:0.966498\ttrain-MAP@7:0.893141\tdval-MAP@7:0.857499\n",
      "[94]\ttrain-mlogloss:0.780128\tdval-mlogloss:0.96512\ttrain-MAP@7:0.893286\tdval-MAP@7:0.857563\n",
      "[95]\ttrain-mlogloss:0.778574\tdval-mlogloss:0.964153\ttrain-MAP@7:0.893374\tdval-MAP@7:0.857584\n",
      "[96]\ttrain-mlogloss:0.776962\tdval-mlogloss:0.962695\ttrain-MAP@7:0.893487\tdval-MAP@7:0.857678\n",
      "[97]\ttrain-mlogloss:0.775399\tdval-mlogloss:0.961222\ttrain-MAP@7:0.8935\tdval-MAP@7:0.85775\n",
      "[98]\ttrain-mlogloss:0.773871\tdval-mlogloss:0.959959\ttrain-MAP@7:0.893474\tdval-MAP@7:0.857943\n",
      "[99]\ttrain-mlogloss:0.772415\tdval-mlogloss:0.95917\ttrain-MAP@7:0.893581\tdval-MAP@7:0.857868\n",
      "[100]\ttrain-mlogloss:0.770977\tdval-mlogloss:0.957982\ttrain-MAP@7:0.8936\tdval-MAP@7:0.857942\n",
      "[101]\ttrain-mlogloss:0.769617\tdval-mlogloss:0.956557\ttrain-MAP@7:0.893653\tdval-MAP@7:0.85825\n",
      "[102]\ttrain-mlogloss:0.768195\tdval-mlogloss:0.955497\ttrain-MAP@7:0.893702\tdval-MAP@7:0.858366\n",
      "[103]\ttrain-mlogloss:0.766882\tdval-mlogloss:0.954452\ttrain-MAP@7:0.893762\tdval-MAP@7:0.858516\n",
      "[104]\ttrain-mlogloss:0.765546\tdval-mlogloss:0.953897\ttrain-MAP@7:0.893879\tdval-MAP@7:0.858604\n",
      "[105]\ttrain-mlogloss:0.764242\tdval-mlogloss:0.952726\ttrain-MAP@7:0.89405\tdval-MAP@7:0.858597\n",
      "[106]\ttrain-mlogloss:0.762984\tdval-mlogloss:0.951704\ttrain-MAP@7:0.894061\tdval-MAP@7:0.858662\n",
      "[107]\ttrain-mlogloss:0.761773\tdval-mlogloss:0.95049\ttrain-MAP@7:0.894119\tdval-MAP@7:0.858796\n",
      "[108]\ttrain-mlogloss:0.760612\tdval-mlogloss:0.94957\ttrain-MAP@7:0.894148\tdval-MAP@7:0.85879\n",
      "[109]\ttrain-mlogloss:0.759398\tdval-mlogloss:0.948535\ttrain-MAP@7:0.894209\tdval-MAP@7:0.858979\n",
      "[110]\ttrain-mlogloss:0.758294\tdval-mlogloss:0.94799\ttrain-MAP@7:0.894254\tdval-MAP@7:0.859105\n",
      "[111]\ttrain-mlogloss:0.75717\tdval-mlogloss:0.947203\ttrain-MAP@7:0.89436\tdval-MAP@7:0.859179\n",
      "[112]\ttrain-mlogloss:0.756049\tdval-mlogloss:0.946455\ttrain-MAP@7:0.894347\tdval-MAP@7:0.859202\n",
      "[113]\ttrain-mlogloss:0.754927\tdval-mlogloss:0.945441\ttrain-MAP@7:0.894443\tdval-MAP@7:0.859263\n",
      "[114]\ttrain-mlogloss:0.75388\tdval-mlogloss:0.944599\ttrain-MAP@7:0.894478\tdval-MAP@7:0.859206\n",
      "[115]\ttrain-mlogloss:0.752881\tdval-mlogloss:0.943988\ttrain-MAP@7:0.894569\tdval-MAP@7:0.859295\n",
      "[116]\ttrain-mlogloss:0.751864\tdval-mlogloss:0.943067\ttrain-MAP@7:0.894615\tdval-MAP@7:0.859381\n",
      "[117]\ttrain-mlogloss:0.750759\tdval-mlogloss:0.942937\ttrain-MAP@7:0.894821\tdval-MAP@7:0.859446\n",
      "[118]\ttrain-mlogloss:0.749734\tdval-mlogloss:0.942145\ttrain-MAP@7:0.894889\tdval-MAP@7:0.859411\n",
      "[119]\ttrain-mlogloss:0.748786\tdval-mlogloss:0.941364\ttrain-MAP@7:0.894998\tdval-MAP@7:0.859526\n",
      "[120]\ttrain-mlogloss:0.74772\tdval-mlogloss:0.940719\ttrain-MAP@7:0.895056\tdval-MAP@7:0.859619\n",
      "[121]\ttrain-mlogloss:0.746726\tdval-mlogloss:0.94012\ttrain-MAP@7:0.895191\tdval-MAP@7:0.859671\n",
      "[122]\ttrain-mlogloss:0.745831\tdval-mlogloss:0.939452\ttrain-MAP@7:0.895168\tdval-MAP@7:0.859661\n",
      "[123]\ttrain-mlogloss:0.744939\tdval-mlogloss:0.938759\ttrain-MAP@7:0.895181\tdval-MAP@7:0.859852\n",
      "[124]\ttrain-mlogloss:0.744117\tdval-mlogloss:0.938201\ttrain-MAP@7:0.895275\tdval-MAP@7:0.859811\n",
      "[125]\ttrain-mlogloss:0.743189\tdval-mlogloss:0.937626\ttrain-MAP@7:0.895291\tdval-MAP@7:0.859745\n",
      "[126]\ttrain-mlogloss:0.742312\tdval-mlogloss:0.936977\ttrain-MAP@7:0.895459\tdval-MAP@7:0.859685\n",
      "[127]\ttrain-mlogloss:0.741494\tdval-mlogloss:0.936754\ttrain-MAP@7:0.895534\tdval-MAP@7:0.859593\n",
      "[128]\ttrain-mlogloss:0.740642\tdval-mlogloss:0.936164\ttrain-MAP@7:0.895658\tdval-MAP@7:0.859651\n",
      "[129]\ttrain-mlogloss:0.739828\tdval-mlogloss:0.935675\ttrain-MAP@7:0.895732\tdval-MAP@7:0.85969\n",
      "[130]\ttrain-mlogloss:0.738948\tdval-mlogloss:0.935223\ttrain-MAP@7:0.895818\tdval-MAP@7:0.859599\n",
      "[131]\ttrain-mlogloss:0.738141\tdval-mlogloss:0.934522\ttrain-MAP@7:0.89585\tdval-MAP@7:0.85976\n",
      "[132]\ttrain-mlogloss:0.737348\tdval-mlogloss:0.934252\ttrain-MAP@7:0.895913\tdval-MAP@7:0.859901\n",
      "[133]\ttrain-mlogloss:0.736566\tdval-mlogloss:0.933809\ttrain-MAP@7:0.895923\tdval-MAP@7:0.85994\n",
      "[134]\ttrain-mlogloss:0.73577\tdval-mlogloss:0.933603\ttrain-MAP@7:0.895969\tdval-MAP@7:0.859907\n",
      "[135]\ttrain-mlogloss:0.735039\tdval-mlogloss:0.932908\ttrain-MAP@7:0.896032\tdval-MAP@7:0.859967\n",
      "[136]\ttrain-mlogloss:0.734311\tdval-mlogloss:0.932658\ttrain-MAP@7:0.896172\tdval-MAP@7:0.85994\n",
      "[137]\ttrain-mlogloss:0.733585\tdval-mlogloss:0.931955\ttrain-MAP@7:0.89631\tdval-MAP@7:0.859954\n",
      "[138]\ttrain-mlogloss:0.732816\tdval-mlogloss:0.931606\ttrain-MAP@7:0.896478\tdval-MAP@7:0.859789\n",
      "[139]\ttrain-mlogloss:0.73207\tdval-mlogloss:0.931197\ttrain-MAP@7:0.896458\tdval-MAP@7:0.859951\n",
      "[140]\ttrain-mlogloss:0.731397\tdval-mlogloss:0.930974\ttrain-MAP@7:0.896516\tdval-MAP@7:0.859986\n",
      "[141]\ttrain-mlogloss:0.730685\tdval-mlogloss:0.930447\ttrain-MAP@7:0.896536\tdval-MAP@7:0.86007\n",
      "[142]\ttrain-mlogloss:0.730013\tdval-mlogloss:0.930033\ttrain-MAP@7:0.896528\tdval-MAP@7:0.860087\n",
      "[143]\ttrain-mlogloss:0.729362\tdval-mlogloss:0.929549\ttrain-MAP@7:0.896607\tdval-MAP@7:0.860141\n",
      "[144]\ttrain-mlogloss:0.728724\tdval-mlogloss:0.928937\ttrain-MAP@7:0.896654\tdval-MAP@7:0.860285\n",
      "[145]\ttrain-mlogloss:0.728089\tdval-mlogloss:0.9287\ttrain-MAP@7:0.896661\tdval-MAP@7:0.860297\n",
      "[146]\ttrain-mlogloss:0.72743\tdval-mlogloss:0.928472\ttrain-MAP@7:0.896759\tdval-MAP@7:0.860295\n",
      "[147]\ttrain-mlogloss:0.726848\tdval-mlogloss:0.928209\ttrain-MAP@7:0.896816\tdval-MAP@7:0.860401\n",
      "[148]\ttrain-mlogloss:0.726183\tdval-mlogloss:0.92769\ttrain-MAP@7:0.89687\tdval-MAP@7:0.860377\n",
      "[149]\ttrain-mlogloss:0.725637\tdval-mlogloss:0.927372\ttrain-MAP@7:0.896965\tdval-MAP@7:0.860475\n",
      "[150]\ttrain-mlogloss:0.725027\tdval-mlogloss:0.927193\ttrain-MAP@7:0.896927\tdval-MAP@7:0.860548\n",
      "[151]\ttrain-mlogloss:0.724429\tdval-mlogloss:0.927018\ttrain-MAP@7:0.897012\tdval-MAP@7:0.860462\n",
      "[152]\ttrain-mlogloss:0.723799\tdval-mlogloss:0.926689\ttrain-MAP@7:0.897061\tdval-MAP@7:0.860405\n",
      "[153]\ttrain-mlogloss:0.723139\tdval-mlogloss:0.926769\ttrain-MAP@7:0.89712\tdval-MAP@7:0.860284\n",
      "[154]\ttrain-mlogloss:0.722532\tdval-mlogloss:0.926403\ttrain-MAP@7:0.897214\tdval-MAP@7:0.860512\n",
      "[155]\ttrain-mlogloss:0.721909\tdval-mlogloss:0.926122\ttrain-MAP@7:0.897189\tdval-MAP@7:0.860525\n",
      "[156]\ttrain-mlogloss:0.721386\tdval-mlogloss:0.925971\ttrain-MAP@7:0.89724\tdval-MAP@7:0.860491\n",
      "[157]\ttrain-mlogloss:0.720818\tdval-mlogloss:0.925725\ttrain-MAP@7:0.89727\tdval-MAP@7:0.860443\n",
      "[158]\ttrain-mlogloss:0.720252\tdval-mlogloss:0.925622\ttrain-MAP@7:0.897341\tdval-MAP@7:0.860494\n",
      "[159]\ttrain-mlogloss:0.719711\tdval-mlogloss:0.92554\ttrain-MAP@7:0.897446\tdval-MAP@7:0.860439\n",
      "[160]\ttrain-mlogloss:0.719187\tdval-mlogloss:0.925367\ttrain-MAP@7:0.897563\tdval-MAP@7:0.860554\n",
      "[161]\ttrain-mlogloss:0.718665\tdval-mlogloss:0.925162\ttrain-MAP@7:0.897645\tdval-MAP@7:0.86052\n",
      "[162]\ttrain-mlogloss:0.718083\tdval-mlogloss:0.924798\ttrain-MAP@7:0.897709\tdval-MAP@7:0.860584\n",
      "[163]\ttrain-mlogloss:0.717473\tdval-mlogloss:0.924462\ttrain-MAP@7:0.897745\tdval-MAP@7:0.860707\n",
      "[164]\ttrain-mlogloss:0.716901\tdval-mlogloss:0.924398\ttrain-MAP@7:0.897784\tdval-MAP@7:0.860646\n",
      "[165]\ttrain-mlogloss:0.716384\tdval-mlogloss:0.924188\ttrain-MAP@7:0.897833\tdval-MAP@7:0.860549\n",
      "[166]\ttrain-mlogloss:0.715847\tdval-mlogloss:0.92389\ttrain-MAP@7:0.898014\tdval-MAP@7:0.860627\n",
      "[167]\ttrain-mlogloss:0.71534\tdval-mlogloss:0.924009\ttrain-MAP@7:0.898077\tdval-MAP@7:0.860646\n",
      "[168]\ttrain-mlogloss:0.714828\tdval-mlogloss:0.924161\ttrain-MAP@7:0.898106\tdval-MAP@7:0.860598\n",
      "[169]\ttrain-mlogloss:0.714316\tdval-mlogloss:0.923908\ttrain-MAP@7:0.898213\tdval-MAP@7:0.860572\n",
      "[170]\ttrain-mlogloss:0.713778\tdval-mlogloss:0.923749\ttrain-MAP@7:0.898279\tdval-MAP@7:0.860513\n",
      "[171]\ttrain-mlogloss:0.713199\tdval-mlogloss:0.923233\ttrain-MAP@7:0.898389\tdval-MAP@7:0.860535\n",
      "[172]\ttrain-mlogloss:0.712718\tdval-mlogloss:0.923015\ttrain-MAP@7:0.898375\tdval-MAP@7:0.860474\n",
      "[173]\ttrain-mlogloss:0.712263\tdval-mlogloss:0.922748\ttrain-MAP@7:0.898526\tdval-MAP@7:0.860476\n",
      "[174]\ttrain-mlogloss:0.711743\tdval-mlogloss:0.92248\ttrain-MAP@7:0.898605\tdval-MAP@7:0.860526\n",
      "[175]\ttrain-mlogloss:0.71121\tdval-mlogloss:0.922251\ttrain-MAP@7:0.898676\tdval-MAP@7:0.860561\n",
      "[176]\ttrain-mlogloss:0.710711\tdval-mlogloss:0.922088\ttrain-MAP@7:0.89875\tdval-MAP@7:0.860566\n",
      "[177]\ttrain-mlogloss:0.710194\tdval-mlogloss:0.921656\ttrain-MAP@7:0.898828\tdval-MAP@7:0.860526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178]\ttrain-mlogloss:0.70972\tdval-mlogloss:0.921897\ttrain-MAP@7:0.898992\tdval-MAP@7:0.860569\n",
      "[179]\ttrain-mlogloss:0.709252\tdval-mlogloss:0.921839\ttrain-MAP@7:0.898981\tdval-MAP@7:0.860568\n",
      "[180]\ttrain-mlogloss:0.708827\tdval-mlogloss:0.921852\ttrain-MAP@7:0.899033\tdval-MAP@7:0.860525\n",
      "[181]\ttrain-mlogloss:0.708313\tdval-mlogloss:0.921785\ttrain-MAP@7:0.899077\tdval-MAP@7:0.860569\n",
      "[182]\ttrain-mlogloss:0.707862\tdval-mlogloss:0.921584\ttrain-MAP@7:0.899111\tdval-MAP@7:0.860594\n",
      "[183]\ttrain-mlogloss:0.707422\tdval-mlogloss:0.921229\ttrain-MAP@7:0.899176\tdval-MAP@7:0.860627\n",
      "[184]\ttrain-mlogloss:0.706978\tdval-mlogloss:0.921073\ttrain-MAP@7:0.899286\tdval-MAP@7:0.860675\n",
      "[185]\ttrain-mlogloss:0.706554\tdval-mlogloss:0.920999\ttrain-MAP@7:0.899251\tdval-MAP@7:0.860582\n",
      "[186]\ttrain-mlogloss:0.706146\tdval-mlogloss:0.921164\ttrain-MAP@7:0.899284\tdval-MAP@7:0.86065\n",
      "[187]\ttrain-mlogloss:0.705745\tdval-mlogloss:0.921153\ttrain-MAP@7:0.89935\tdval-MAP@7:0.860631\n",
      "[188]\ttrain-mlogloss:0.705303\tdval-mlogloss:0.920961\ttrain-MAP@7:0.899431\tdval-MAP@7:0.860619\n",
      "[189]\ttrain-mlogloss:0.704789\tdval-mlogloss:0.9212\ttrain-MAP@7:0.899492\tdval-MAP@7:0.860553\n",
      "[190]\ttrain-mlogloss:0.704342\tdval-mlogloss:0.92118\ttrain-MAP@7:0.899652\tdval-MAP@7:0.860594\n",
      "[191]\ttrain-mlogloss:0.703934\tdval-mlogloss:0.921123\ttrain-MAP@7:0.899729\tdval-MAP@7:0.860601\n",
      "[192]\ttrain-mlogloss:0.703498\tdval-mlogloss:0.921032\ttrain-MAP@7:0.899779\tdval-MAP@7:0.860679\n",
      "[193]\ttrain-mlogloss:0.703106\tdval-mlogloss:0.921217\ttrain-MAP@7:0.899883\tdval-MAP@7:0.860645\n",
      "[194]\ttrain-mlogloss:0.702695\tdval-mlogloss:0.921424\ttrain-MAP@7:0.899905\tdval-MAP@7:0.860566\n",
      "[195]\ttrain-mlogloss:0.702232\tdval-mlogloss:0.92124\ttrain-MAP@7:0.900034\tdval-MAP@7:0.860567\n",
      "[196]\ttrain-mlogloss:0.701759\tdval-mlogloss:0.921216\ttrain-MAP@7:0.900125\tdval-MAP@7:0.860603\n",
      "[197]\ttrain-mlogloss:0.701286\tdval-mlogloss:0.92111\ttrain-MAP@7:0.900115\tdval-MAP@7:0.860625\n",
      "[198]\ttrain-mlogloss:0.700909\tdval-mlogloss:0.920939\ttrain-MAP@7:0.900203\tdval-MAP@7:0.860617\n",
      "[199]\ttrain-mlogloss:0.700475\tdval-mlogloss:0.920743\ttrain-MAP@7:0.900314\tdval-MAP@7:0.860605\n",
      "[200]\ttrain-mlogloss:0.700035\tdval-mlogloss:0.921072\ttrain-MAP@7:0.900413\tdval-MAP@7:0.860607\n",
      "[201]\ttrain-mlogloss:0.699584\tdval-mlogloss:0.921112\ttrain-MAP@7:0.900516\tdval-MAP@7:0.860494\n",
      "[202]\ttrain-mlogloss:0.699198\tdval-mlogloss:0.921077\ttrain-MAP@7:0.900574\tdval-MAP@7:0.860437\n",
      "[203]\ttrain-mlogloss:0.698798\tdval-mlogloss:0.92125\ttrain-MAP@7:0.900635\tdval-MAP@7:0.860383\n",
      "[204]\ttrain-mlogloss:0.698364\tdval-mlogloss:0.921236\ttrain-MAP@7:0.900741\tdval-MAP@7:0.860297\n",
      "[205]\ttrain-mlogloss:0.697975\tdval-mlogloss:0.921365\ttrain-MAP@7:0.900783\tdval-MAP@7:0.860308\n",
      "[206]\ttrain-mlogloss:0.697499\tdval-mlogloss:0.921431\ttrain-MAP@7:0.900818\tdval-MAP@7:0.860395\n",
      "[207]\ttrain-mlogloss:0.69708\tdval-mlogloss:0.921188\ttrain-MAP@7:0.900862\tdval-MAP@7:0.860469\n",
      "[208]\ttrain-mlogloss:0.696642\tdval-mlogloss:0.921368\ttrain-MAP@7:0.900984\tdval-MAP@7:0.860352\n",
      "[209]\ttrain-mlogloss:0.69617\tdval-mlogloss:0.921319\ttrain-MAP@7:0.901068\tdval-MAP@7:0.860344\n",
      "[210]\ttrain-mlogloss:0.695757\tdval-mlogloss:0.921334\ttrain-MAP@7:0.901159\tdval-MAP@7:0.860396\n",
      "[211]\ttrain-mlogloss:0.695345\tdval-mlogloss:0.92159\ttrain-MAP@7:0.901257\tdval-MAP@7:0.860307\n",
      "[212]\ttrain-mlogloss:0.694946\tdval-mlogloss:0.921186\ttrain-MAP@7:0.901314\tdval-MAP@7:0.860278\n",
      "[213]\ttrain-mlogloss:0.694578\tdval-mlogloss:0.9211\ttrain-MAP@7:0.901325\tdval-MAP@7:0.860286\n",
      "[214]\ttrain-mlogloss:0.694194\tdval-mlogloss:0.921054\ttrain-MAP@7:0.901467\tdval-MAP@7:0.860277\n",
      "[215]\ttrain-mlogloss:0.69377\tdval-mlogloss:0.921219\ttrain-MAP@7:0.901467\tdval-MAP@7:0.860101\n",
      "[216]\ttrain-mlogloss:0.693388\tdval-mlogloss:0.921075\ttrain-MAP@7:0.901526\tdval-MAP@7:0.860112\n",
      "[217]\ttrain-mlogloss:0.693007\tdval-mlogloss:0.921085\ttrain-MAP@7:0.901648\tdval-MAP@7:0.860094\n",
      "[218]\ttrain-mlogloss:0.692638\tdval-mlogloss:0.920953\ttrain-MAP@7:0.901681\tdval-MAP@7:0.860144\n",
      "[219]\ttrain-mlogloss:0.692313\tdval-mlogloss:0.920923\ttrain-MAP@7:0.901738\tdval-MAP@7:0.860194\n",
      "[220]\ttrain-mlogloss:0.691968\tdval-mlogloss:0.920882\ttrain-MAP@7:0.901794\tdval-MAP@7:0.860137\n",
      "[221]\ttrain-mlogloss:0.691606\tdval-mlogloss:0.920909\ttrain-MAP@7:0.901916\tdval-MAP@7:0.860156\n",
      "[222]\ttrain-mlogloss:0.691255\tdval-mlogloss:0.921\ttrain-MAP@7:0.901994\tdval-MAP@7:0.860131\n",
      "[223]\ttrain-mlogloss:0.69083\tdval-mlogloss:0.92089\ttrain-MAP@7:0.902015\tdval-MAP@7:0.860133\n",
      "[224]\ttrain-mlogloss:0.690473\tdval-mlogloss:0.921003\ttrain-MAP@7:0.902131\tdval-MAP@7:0.860137\n",
      "[225]\ttrain-mlogloss:0.690097\tdval-mlogloss:0.921007\ttrain-MAP@7:0.902208\tdval-MAP@7:0.860125\n",
      "[226]\ttrain-mlogloss:0.689704\tdval-mlogloss:0.920984\ttrain-MAP@7:0.902241\tdval-MAP@7:0.860123\n",
      "[227]\ttrain-mlogloss:0.689313\tdval-mlogloss:0.920981\ttrain-MAP@7:0.902396\tdval-MAP@7:0.860011\n",
      "[228]\ttrain-mlogloss:0.688994\tdval-mlogloss:0.920896\ttrain-MAP@7:0.902448\tdval-MAP@7:0.860113\n",
      "[229]\ttrain-mlogloss:0.688664\tdval-mlogloss:0.921088\ttrain-MAP@7:0.902552\tdval-MAP@7:0.859983\n",
      "[230]\ttrain-mlogloss:0.688346\tdval-mlogloss:0.921133\ttrain-MAP@7:0.90262\tdval-MAP@7:0.860013\n",
      "[231]\ttrain-mlogloss:0.687923\tdval-mlogloss:0.92122\ttrain-MAP@7:0.902675\tdval-MAP@7:0.859981\n",
      "[232]\ttrain-mlogloss:0.687567\tdval-mlogloss:0.921233\ttrain-MAP@7:0.902857\tdval-MAP@7:0.85988\n",
      "[233]\ttrain-mlogloss:0.687205\tdval-mlogloss:0.921043\ttrain-MAP@7:0.902811\tdval-MAP@7:0.859947\n",
      "[234]\ttrain-mlogloss:0.686838\tdval-mlogloss:0.921158\ttrain-MAP@7:0.902909\tdval-MAP@7:0.859877\n",
      "[235]\ttrain-mlogloss:0.686534\tdval-mlogloss:0.920933\ttrain-MAP@7:0.902969\tdval-MAP@7:0.859919\n",
      "[236]\ttrain-mlogloss:0.686172\tdval-mlogloss:0.920746\ttrain-MAP@7:0.903014\tdval-MAP@7:0.859848\n",
      "[237]\ttrain-mlogloss:0.685726\tdval-mlogloss:0.920674\ttrain-MAP@7:0.903151\tdval-MAP@7:0.859832\n",
      "[238]\ttrain-mlogloss:0.685382\tdval-mlogloss:0.920895\ttrain-MAP@7:0.903155\tdval-MAP@7:0.859777\n",
      "[239]\ttrain-mlogloss:0.685011\tdval-mlogloss:0.920906\ttrain-MAP@7:0.903216\tdval-MAP@7:0.859821\n",
      "[240]\ttrain-mlogloss:0.684706\tdval-mlogloss:0.92086\ttrain-MAP@7:0.90339\tdval-MAP@7:0.859724\n",
      "[241]\ttrain-mlogloss:0.684365\tdval-mlogloss:0.920923\ttrain-MAP@7:0.90343\tdval-MAP@7:0.85984\n",
      "[242]\ttrain-mlogloss:0.684056\tdval-mlogloss:0.920983\ttrain-MAP@7:0.903422\tdval-MAP@7:0.859825\n",
      "[243]\ttrain-mlogloss:0.683689\tdval-mlogloss:0.921142\ttrain-MAP@7:0.903536\tdval-MAP@7:0.859817\n",
      "[244]\ttrain-mlogloss:0.683291\tdval-mlogloss:0.920901\ttrain-MAP@7:0.903595\tdval-MAP@7:0.859896\n",
      "[245]\ttrain-mlogloss:0.682941\tdval-mlogloss:0.921111\ttrain-MAP@7:0.903566\tdval-MAP@7:0.859841\n",
      "[246]\ttrain-mlogloss:0.682581\tdval-mlogloss:0.920975\ttrain-MAP@7:0.903701\tdval-MAP@7:0.859945\n",
      "[247]\ttrain-mlogloss:0.682221\tdval-mlogloss:0.921233\ttrain-MAP@7:0.903818\tdval-MAP@7:0.859773\n",
      "[248]\ttrain-mlogloss:0.681853\tdval-mlogloss:0.921166\ttrain-MAP@7:0.903868\tdval-MAP@7:0.859768\n",
      "[249]\ttrain-mlogloss:0.681491\tdval-mlogloss:0.921252\ttrain-MAP@7:0.904004\tdval-MAP@7:0.859758\n",
      "[250]\ttrain-mlogloss:0.681122\tdval-mlogloss:0.921209\ttrain-MAP@7:0.90401\tdval-MAP@7:0.859761\n",
      "[251]\ttrain-mlogloss:0.680804\tdval-mlogloss:0.921224\ttrain-MAP@7:0.904175\tdval-MAP@7:0.859813\n",
      "[252]\ttrain-mlogloss:0.680455\tdval-mlogloss:0.92132\ttrain-MAP@7:0.904295\tdval-MAP@7:0.859824\n",
      "[253]\ttrain-mlogloss:0.68014\tdval-mlogloss:0.92124\ttrain-MAP@7:0.904318\tdval-MAP@7:0.859836\n",
      "[254]\ttrain-mlogloss:0.679801\tdval-mlogloss:0.92127\ttrain-MAP@7:0.904348\tdval-MAP@7:0.859809\n",
      "[255]\ttrain-mlogloss:0.679478\tdval-mlogloss:0.921234\ttrain-MAP@7:0.904375\tdval-MAP@7:0.859859\n",
      "[256]\ttrain-mlogloss:0.679184\tdval-mlogloss:0.921419\ttrain-MAP@7:0.904434\tdval-MAP@7:0.859621\n",
      "[257]\ttrain-mlogloss:0.678861\tdval-mlogloss:0.921625\ttrain-MAP@7:0.9045\tdval-MAP@7:0.859674\n",
      "[258]\ttrain-mlogloss:0.67855\tdval-mlogloss:0.921711\ttrain-MAP@7:0.90457\tdval-MAP@7:0.859544\n",
      "[259]\ttrain-mlogloss:0.678172\tdval-mlogloss:0.921922\ttrain-MAP@7:0.904641\tdval-MAP@7:0.859571\n",
      "[260]\ttrain-mlogloss:0.677846\tdval-mlogloss:0.921972\ttrain-MAP@7:0.904748\tdval-MAP@7:0.859621\n",
      "[261]\ttrain-mlogloss:0.677588\tdval-mlogloss:0.922168\ttrain-MAP@7:0.90475\tdval-MAP@7:0.859588\n",
      "[262]\ttrain-mlogloss:0.677216\tdval-mlogloss:0.922675\ttrain-MAP@7:0.904766\tdval-MAP@7:0.859519\n",
      "[263]\ttrain-mlogloss:0.676886\tdval-mlogloss:0.922493\ttrain-MAP@7:0.904822\tdval-MAP@7:0.859515\n",
      "[264]\ttrain-mlogloss:0.676561\tdval-mlogloss:0.922418\ttrain-MAP@7:0.90484\tdval-MAP@7:0.859478\n",
      "[265]\ttrain-mlogloss:0.676242\tdval-mlogloss:0.922393\ttrain-MAP@7:0.904846\tdval-MAP@7:0.85953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266]\ttrain-mlogloss:0.675915\tdval-mlogloss:0.922529\ttrain-MAP@7:0.904943\tdval-MAP@7:0.859399\n",
      "[267]\ttrain-mlogloss:0.675593\tdval-mlogloss:0.922575\ttrain-MAP@7:0.905059\tdval-MAP@7:0.859422\n",
      "[268]\ttrain-mlogloss:0.675274\tdval-mlogloss:0.922472\ttrain-MAP@7:0.905087\tdval-MAP@7:0.859524\n",
      "[269]\ttrain-mlogloss:0.674933\tdval-mlogloss:0.922565\ttrain-MAP@7:0.905126\tdval-MAP@7:0.859382\n",
      "[270]\ttrain-mlogloss:0.674614\tdval-mlogloss:0.92268\ttrain-MAP@7:0.905215\tdval-MAP@7:0.859304\n",
      "[271]\ttrain-mlogloss:0.674254\tdval-mlogloss:0.922714\ttrain-MAP@7:0.905297\tdval-MAP@7:0.859307\n",
      "[272]\ttrain-mlogloss:0.673892\tdval-mlogloss:0.922721\ttrain-MAP@7:0.905296\tdval-MAP@7:0.859278\n",
      "[273]\ttrain-mlogloss:0.67357\tdval-mlogloss:0.922665\ttrain-MAP@7:0.90537\tdval-MAP@7:0.859279\n",
      "[274]\ttrain-mlogloss:0.673229\tdval-mlogloss:0.922602\ttrain-MAP@7:0.905377\tdval-MAP@7:0.859315\n",
      "[275]\ttrain-mlogloss:0.672906\tdval-mlogloss:0.922704\ttrain-MAP@7:0.905451\tdval-MAP@7:0.859332\n",
      "[276]\ttrain-mlogloss:0.672606\tdval-mlogloss:0.922956\ttrain-MAP@7:0.905611\tdval-MAP@7:0.859272\n",
      "[277]\ttrain-mlogloss:0.67231\tdval-mlogloss:0.922996\ttrain-MAP@7:0.905626\tdval-MAP@7:0.859312\n",
      "[278]\ttrain-mlogloss:0.671962\tdval-mlogloss:0.922908\ttrain-MAP@7:0.905656\tdval-MAP@7:0.859335\n",
      "[279]\ttrain-mlogloss:0.671661\tdval-mlogloss:0.922995\ttrain-MAP@7:0.905751\tdval-MAP@7:0.859204\n",
      "[280]\ttrain-mlogloss:0.671356\tdval-mlogloss:0.923092\ttrain-MAP@7:0.905872\tdval-MAP@7:0.859201\n",
      "[281]\ttrain-mlogloss:0.671048\tdval-mlogloss:0.923083\ttrain-MAP@7:0.905938\tdval-MAP@7:0.859171\n",
      "[282]\ttrain-mlogloss:0.670781\tdval-mlogloss:0.923114\ttrain-MAP@7:0.905983\tdval-MAP@7:0.859175\n",
      "[283]\ttrain-mlogloss:0.670473\tdval-mlogloss:0.923372\ttrain-MAP@7:0.906075\tdval-MAP@7:0.859032\n",
      "[284]\ttrain-mlogloss:0.670167\tdval-mlogloss:0.92347\ttrain-MAP@7:0.906207\tdval-MAP@7:0.858887\n",
      "[285]\ttrain-mlogloss:0.669858\tdval-mlogloss:0.923553\ttrain-MAP@7:0.906259\tdval-MAP@7:0.858992\n",
      "[286]\ttrain-mlogloss:0.66962\tdval-mlogloss:0.923743\ttrain-MAP@7:0.906374\tdval-MAP@7:0.858935\n",
      "[287]\ttrain-mlogloss:0.669338\tdval-mlogloss:0.923624\ttrain-MAP@7:0.90635\tdval-MAP@7:0.859135\n",
      "[288]\ttrain-mlogloss:0.669004\tdval-mlogloss:0.923762\ttrain-MAP@7:0.906382\tdval-MAP@7:0.85914\n",
      "[289]\ttrain-mlogloss:0.668696\tdval-mlogloss:0.923769\ttrain-MAP@7:0.906513\tdval-MAP@7:0.859187\n",
      "[290]\ttrain-mlogloss:0.668439\tdval-mlogloss:0.923718\ttrain-MAP@7:0.906526\tdval-MAP@7:0.859132\n",
      "[291]\ttrain-mlogloss:0.668144\tdval-mlogloss:0.924123\ttrain-MAP@7:0.906563\tdval-MAP@7:0.858896\n",
      "[292]\ttrain-mlogloss:0.667853\tdval-mlogloss:0.924073\ttrain-MAP@7:0.906605\tdval-MAP@7:0.858878\n",
      "[293]\ttrain-mlogloss:0.667538\tdval-mlogloss:0.924137\ttrain-MAP@7:0.906654\tdval-MAP@7:0.858947\n",
      "[294]\ttrain-mlogloss:0.667264\tdval-mlogloss:0.924604\ttrain-MAP@7:0.906703\tdval-MAP@7:0.858661\n",
      "[295]\ttrain-mlogloss:0.666984\tdval-mlogloss:0.924876\ttrain-MAP@7:0.906745\tdval-MAP@7:0.858659\n",
      "[296]\ttrain-mlogloss:0.666635\tdval-mlogloss:0.925095\ttrain-MAP@7:0.906819\tdval-MAP@7:0.858539\n",
      "[297]\ttrain-mlogloss:0.666339\tdval-mlogloss:0.925127\ttrain-MAP@7:0.906816\tdval-MAP@7:0.858539\n",
      "[298]\ttrain-mlogloss:0.666021\tdval-mlogloss:0.925127\ttrain-MAP@7:0.906933\tdval-MAP@7:0.858499\n",
      "[299]\ttrain-mlogloss:0.665733\tdval-mlogloss:0.925138\ttrain-MAP@7:0.906956\tdval-MAP@7:0.858484\n",
      "[0]\ttrain-mlogloss:2.68853\tdval-mlogloss:2.71612\ttrain-MAP@7:0.870197\tdval-MAP@7:0.827242\n",
      "[1]\ttrain-mlogloss:2.50948\tdval-mlogloss:2.55043\ttrain-MAP@7:0.874393\tdval-MAP@7:0.840052\n",
      "[2]\ttrain-mlogloss:2.36727\tdval-mlogloss:2.4193\ttrain-MAP@7:0.875426\tdval-MAP@7:0.839752\n",
      "[3]\ttrain-mlogloss:2.24981\tdval-mlogloss:2.31119\ttrain-MAP@7:0.876688\tdval-MAP@7:0.839122\n",
      "[4]\ttrain-mlogloss:2.14967\tdval-mlogloss:2.22357\ttrain-MAP@7:0.877723\tdval-MAP@7:0.838217\n",
      "[5]\ttrain-mlogloss:2.06232\tdval-mlogloss:2.14174\ttrain-MAP@7:0.878175\tdval-MAP@7:0.839832\n",
      "[6]\ttrain-mlogloss:1.98454\tdval-mlogloss:2.0698\ttrain-MAP@7:0.878938\tdval-MAP@7:0.841593\n",
      "[7]\ttrain-mlogloss:1.91458\tdval-mlogloss:2.00539\ttrain-MAP@7:0.879544\tdval-MAP@7:0.842008\n",
      "[8]\ttrain-mlogloss:1.85185\tdval-mlogloss:1.94818\ttrain-MAP@7:0.88012\tdval-MAP@7:0.843257\n",
      "[9]\ttrain-mlogloss:1.79484\tdval-mlogloss:1.89507\ttrain-MAP@7:0.880886\tdval-MAP@7:0.843574\n",
      "[10]\ttrain-mlogloss:1.74222\tdval-mlogloss:1.84804\ttrain-MAP@7:0.88135\tdval-MAP@7:0.844419\n",
      "[11]\ttrain-mlogloss:1.69383\tdval-mlogloss:1.80353\ttrain-MAP@7:0.881601\tdval-MAP@7:0.845069\n",
      "[12]\ttrain-mlogloss:1.64888\tdval-mlogloss:1.76176\ttrain-MAP@7:0.88215\tdval-MAP@7:0.8453\n",
      "[13]\ttrain-mlogloss:1.60694\tdval-mlogloss:1.72245\ttrain-MAP@7:0.882578\tdval-MAP@7:0.846453\n",
      "[14]\ttrain-mlogloss:1.56809\tdval-mlogloss:1.68616\ttrain-MAP@7:0.882834\tdval-MAP@7:0.846649\n",
      "[15]\ttrain-mlogloss:1.53165\tdval-mlogloss:1.65393\ttrain-MAP@7:0.883011\tdval-MAP@7:0.847076\n",
      "[16]\ttrain-mlogloss:1.49753\tdval-mlogloss:1.6223\ttrain-MAP@7:0.88307\tdval-MAP@7:0.846955\n",
      "[17]\ttrain-mlogloss:1.46565\tdval-mlogloss:1.59275\ttrain-MAP@7:0.883329\tdval-MAP@7:0.847736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b0de6d0712f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     model = xgb.train(param, dtrain, num_rounds, evals=[(dtrain, 'train'), (dval, 'dval')], \n\u001b[1;32m---> 33\u001b[1;33m         verbose_eval=True, feval=eval_map, evals_result=history[weight_index], gt=ground_truth, ts=train_size)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates, **kwargs_eval)\u001b[0m\n\u001b[0;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks, **kwargs_eval)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, **kwargs_eval)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# check evaluation result.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mbst_eval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst_eval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst_eval_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36meval_set\u001b[1;34m(self, evals, iteration, feval, **kwargs_eval)\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m                 \u001b[0mfeval_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdmat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    960\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeval_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeval_ret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m     \"\"\"\n\u001b[1;32m--> 973\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argsort'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\kaggle\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = {}\n",
    "\n",
    "x_train, y_train, weight_train = create_train('2015-06-28', pattern_flag=True)\n",
    "x_val, y_val, weight_val = create_train('2016-05-28', pattern_flag=True)\n",
    "\n",
    "gt_train = prep_map(x_train, y_train)\n",
    "gt_val = prep_map(x_val, y_val)\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dval = xgb.DMatrix(x_val, y_val)\n",
    "\n",
    "ground_truth = {'train': gt_train, 'val': gt_val}\n",
    "data_hash = {'train': hash(dtrain.get_label().tostring()), 'val': hash(dval.get_label().tostring())}\n",
    "\n",
    "for weight_index in [0, 1]:\n",
    "    history[weight_index] = {}\n",
    "    \n",
    "    dtrain.set_weight(weight_train.values[:, weight_index])\n",
    "    dval.set_weight(weight_val.values[:, weight_index])\n",
    "\n",
    "    param = {'objective': 'multi:softprob', \n",
    "             'eta': 0.05, \n",
    "             'max_depth': 4, \n",
    "             'silent': 1, \n",
    "             'num_class': len(target_cols),\n",
    "             'eval_metric': 'mlogloss',\n",
    "             'min_child_weight': 1,\n",
    "             'subsample': 0.7,\n",
    "             'colsample_bytree': 0.7,\n",
    "             'seed': 0}\n",
    "    num_rounds = 300\n",
    "\n",
    "    model = xgb.train(param, dtrain, num_rounds, evals=[(dtrain, 'train'), (dval, 'dval')], \n",
    "        verbose_eval=True, feval=eval_map, evals_result=history[weight_index], gt=ground_truth, ts=data_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-05-28, max_lag=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.64321\n",
      "[1]\ttrain-mlogloss:2.42916\n",
      "[2]\ttrain-mlogloss:2.26143\n",
      "[3]\ttrain-mlogloss:2.12057\n",
      "[4]\ttrain-mlogloss:2.00099\n",
      "[5]\ttrain-mlogloss:1.89603\n",
      "[6]\ttrain-mlogloss:1.80356\n",
      "[7]\ttrain-mlogloss:1.72067\n",
      "[8]\ttrain-mlogloss:1.64465\n",
      "[9]\ttrain-mlogloss:1.57663\n",
      "[10]\ttrain-mlogloss:1.51355\n",
      "[11]\ttrain-mlogloss:1.45557\n",
      "[12]\ttrain-mlogloss:1.40214\n",
      "[13]\ttrain-mlogloss:1.35231\n",
      "[14]\ttrain-mlogloss:1.30587\n",
      "[15]\ttrain-mlogloss:1.26249\n",
      "[16]\ttrain-mlogloss:1.222\n",
      "[17]\ttrain-mlogloss:1.18383\n",
      "[18]\ttrain-mlogloss:1.14811\n",
      "[19]\ttrain-mlogloss:1.11448\n",
      "[20]\ttrain-mlogloss:1.08268\n",
      "[21]\ttrain-mlogloss:1.0525\n",
      "[22]\ttrain-mlogloss:1.02404\n",
      "[23]\ttrain-mlogloss:0.997108\n",
      "[24]\ttrain-mlogloss:0.971606\n",
      "[25]\ttrain-mlogloss:0.947348\n",
      "[26]\ttrain-mlogloss:0.924398\n",
      "[27]\ttrain-mlogloss:0.902768\n",
      "[28]\ttrain-mlogloss:0.881792\n",
      "[29]\ttrain-mlogloss:0.861978\n",
      "[30]\ttrain-mlogloss:0.842969\n",
      "[31]\ttrain-mlogloss:0.824979\n",
      "[32]\ttrain-mlogloss:0.80799\n",
      "[33]\ttrain-mlogloss:0.791558\n",
      "[34]\ttrain-mlogloss:0.776193\n",
      "[35]\ttrain-mlogloss:0.761276\n",
      "[36]\ttrain-mlogloss:0.746991\n",
      "[37]\ttrain-mlogloss:0.733397\n",
      "[38]\ttrain-mlogloss:0.720404\n",
      "[39]\ttrain-mlogloss:0.707905\n",
      "[40]\ttrain-mlogloss:0.696077\n",
      "[41]\ttrain-mlogloss:0.684611\n",
      "[42]\ttrain-mlogloss:0.673712\n",
      "[43]\ttrain-mlogloss:0.66345\n",
      "[44]\ttrain-mlogloss:0.653513\n",
      "[45]\ttrain-mlogloss:0.643782\n",
      "[46]\ttrain-mlogloss:0.634649\n",
      "[47]\ttrain-mlogloss:0.625692\n",
      "[48]\ttrain-mlogloss:0.617063\n",
      "[49]\ttrain-mlogloss:0.608998\n",
      "[50]\ttrain-mlogloss:0.60114\n",
      "[51]\ttrain-mlogloss:0.593598\n",
      "[52]\ttrain-mlogloss:0.586282\n",
      "[53]\ttrain-mlogloss:0.57932\n",
      "[54]\ttrain-mlogloss:0.57262\n",
      "[55]\ttrain-mlogloss:0.566129\n",
      "[56]\ttrain-mlogloss:0.560005\n",
      "[57]\ttrain-mlogloss:0.554003\n",
      "[58]\ttrain-mlogloss:0.548272\n",
      "[59]\ttrain-mlogloss:0.542817\n"
     ]
    }
   ],
   "source": [
    "x_train_may16, y_train_may16 = create_train('2015-05-28', pattern_flag=True, max_lag=16)\n",
    "\n",
    "x_test_may16 = create_test(pattern_flag=True)\n",
    "\n",
    "param = {'objective': 'multi:softprob', \n",
    "         'eta': 0.05, \n",
    "         'max_depth': 8, \n",
    "         'silent': 1, \n",
    "         'num_class': len(target_cols),\n",
    "         'eval_metric': 'mlogloss',\n",
    "         'min_child_weight': 1,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'seed': 0}\n",
    "num_rounds = 60\n",
    "\n",
    "dtrain_may16 = xgb.DMatrix(x_train_may16.values, y_train_may16.values)\n",
    "model_may16 = xgb.train(param, dtrain_may16, num_rounds, evals=[(dtrain_may16, 'train')], verbose_eval=True)\n",
    "\n",
    "preds_may16 = model_may16.predict(xgb.DMatrix(x_test_may16.values))\n",
    "\n",
    "df_preds_may16 = pd.DataFrame(preds_may16, index=x_test_may16.index, columns=target_cols)\n",
    "# Remove already bought products \n",
    "df_preds_may16[x_test_may16[target_cols]==1] = 0 \n",
    "preds_may16 = df_preds_may16.values\n",
    "preds_may16 = np.argsort(preds_may16, axis=1)\n",
    "preds_may16 = np.fliplr(preds_may16)[:, :7]\n",
    "\n",
    "test_id = x_test_may16.loc[:, 'ncodpers'].values\n",
    "final_preds_may16 = [' '.join([target_cols[k] for k in pred]) for pred in preds_may16]\n",
    "\n",
    "out_df_may16 = pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds_may16})\n",
    "out_df_may16.to_csv('eda_4_28_may16.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_june15 = model_june15.predict(xgb.DMatrix(x_test_june15.values))\n",
    "preds_may16 = model_may16.predict(xgb.DMatrix(x_test_may16.values))\n",
    "\n",
    "preds1 = np.sqrt(preds_june15*preds_may16)\n",
    "preds2 = 0.5*preds_june15 + 0.5*preds_may16\n",
    "\n",
    "# Geometry mean\n",
    "df_preds1 = pd.DataFrame(preds1, index=x_test_may16.index, columns=target_cols)\n",
    "# Remove already bought products \n",
    "df_preds1[x_test_may16[target_cols]==1] = 0 \n",
    "preds1 = df_preds1.values\n",
    "preds1 = np.argsort(preds1, axis=1)\n",
    "preds1 = np.fliplr(preds1)[:, :7]\n",
    "\n",
    "test_id = x_test_may16.loc[:, 'ncodpers'].values\n",
    "final_preds1 = [' '.join([target_cols[k] for k in pred]) for pred in preds1]\n",
    "\n",
    "out_df1 = pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds1})\n",
    "out_df1.to_csv('eda_4_28_gm.csv.gz', compression='gzip', index=False)\n",
    "\n",
    "# Algorithmic mean\n",
    "df_preds2 = pd.DataFrame(preds2, index=x_test_may16.index, columns=target_cols)\n",
    "# Remove already bought products \n",
    "df_preds2[x_test_may16[target_cols]==1] = 0 \n",
    "preds2 = df_preds2.values\n",
    "preds2 = np.argsort(preds2, axis=1)\n",
    "preds2 = np.fliplr(preds2)[:, :7]\n",
    "\n",
    "test_id = x_test_may16.loc[:, 'ncodpers'].values\n",
    "final_preds2 = [' '.join([target_cols[k] for k in pred]) for pred in preds2]\n",
    "\n",
    "out_df2 = pd.DataFrame({'ncodpers': test_id, 'added_products': final_preds2})\n",
    "out_df2.to_csv('eda_4_28_am.csv.gz', compression='gzip', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
